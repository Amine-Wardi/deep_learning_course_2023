{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1502bbeee981ecc3a7a4464df92be0c",
     "grade": false,
     "grade_id": "cell-a975d7bfb06f30ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP3 - Recurrent neural networks\n",
    "\n",
    "Credits: Andrej Karpathy\n",
    "\n",
    "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() # For macOS\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing\n",
    "\n",
    "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `shakespeare.txt` contains 10000 characters.\n",
      "Excerpt of the dataset:\n",
      "    SONNETS\n",
      "\n",
      "\n",
      "\n",
      "TO THE ONLY BEGETTER OF\n",
      "THESE INSUING SONNETS\n",
      "MR. W. H. ALL HAPPINESS\n",
      "AND THAT ETERNITY\n",
      "PROMISED BY\n",
      "OUR EVER-LIVING POET WISHETH\n",
      "THE WELL-WISHING\n",
      "ADVENTURER IN\n",
      "SETTING FORTH\n",
      "T. T.\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "FROM fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light'st flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel.\n",
      "Thou that art now the world's fresh ornament\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content\n",
      "And, tender churl, makest waste in niggarding.\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world's due, by the grave and thee.\n",
      "\n",
      "II.\n",
      "\n",
      "When forty winters shall beseige thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery, so gazed on now,\n",
      "Will be a tatter'd weed, of small worth held:\n",
      "Then being ask'd where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days,\n",
      "To say, within thine own deep-sunken eyes,\n",
      "Were an all-eating shame and thriftless praise.\n",
      "How much more praise deserved thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count and make my old excuse,'\n",
      "Proving his beauty by succession thine!\n",
      "  This were to be new made when thou art old,\n",
      "  And see thy blood warm when thou feel'st it cold.\n",
      "\n",
      "III.\n",
      "\n",
      "Look in thy glass, and tell the face thou viewest\n",
      "Now is the time that face should form another;\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose unear'd womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb\n",
      "Of his self-love, to stop posterity?\n",
      "Thou art thy mother's glass, and she in thee\n",
      "Calls back the lovely April of her prime:\n",
      "So thou through windows of thine age shall see\n",
      "Despite of wrinkles this thy golden time.\n",
      "  But if thou\n"
     ]
    }
   ],
   "source": [
    "dir_datasets = \"./text-datasets/\"\n",
    "# text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
    "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
    "text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
    "# text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
    "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
    "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
    "\n",
    "text_data = open(dir_datasets+text_data_fname, 'r').read()\n",
    "text_data = text_data[:10000] # use a small subset\n",
    "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
    "print('Excerpt of the dataset:')\n",
    "print(text_data[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b030743489f86ece30d79835434297",
     "grade": false,
     "grade_id": "cell-9695c6f2cf95337c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885ecc8a-c654-463b-9353-f7a18a607199",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3eb46710b4006993f4eb9c557a2376f",
     "grade": true,
     "grade_id": "vocabulary",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'S', 'O', 'N', 'E', 'T', '\\n', 'H', 'L', 'Y', 'B', 'G', 'R', 'F', 'I', 'U', 'M', '.', 'W', 'A', 'P', 'D', 'V', '-', 'f', 'a', 'i', 'r', 'e', 's', 't', 'c', 'u', 'w', 'd', 'n', ',', 'h', 'b', 'y', \"'\", 'o', 'm', 'g', 'v', 'p', 'l', ':', 'k', 'z', 'x', '!', ';', '?', 'C', 'q', 'j', 'X']\n",
      "{' ': 0, 'S': 1, 'O': 2, 'N': 3, 'E': 4, 'T': 5, '\\n': 6, 'H': 7, 'L': 8, 'Y': 9, 'B': 10, 'G': 11, 'R': 12, 'F': 13, 'I': 14, 'U': 15, 'M': 16, '.': 17, 'W': 18, 'A': 19, 'P': 20, 'D': 21, 'V': 22, '-': 23, 'f': 24, 'a': 25, 'i': 26, 'r': 27, 'e': 28, 's': 29, 't': 30, 'c': 31, 'u': 32, 'w': 33, 'd': 34, 'n': 35, ',': 36, 'h': 37, 'b': 38, 'y': 39, \"'\": 40, 'o': 41, 'm': 42, 'g': 43, 'v': 44, 'p': 45, 'l': 46, ':': 47, 'k': 48, 'z': 49, 'x': 50, '!': 51, ';': 52, '?': 53, 'C': 54, 'q': 55, 'j': 56, 'X': 57}\n",
      "{0: ' ', 1: 'S', 2: 'O', 3: 'N', 4: 'E', 5: 'T', 6: '\\n', 7: 'H', 8: 'L', 9: 'Y', 10: 'B', 11: 'G', 12: 'R', 13: 'F', 14: 'I', 15: 'U', 16: 'M', 17: '.', 18: 'W', 19: 'A', 20: 'P', 21: 'D', 22: 'V', 23: '-', 24: 'f', 25: 'a', 26: 'i', 27: 'r', 28: 'e', 29: 's', 30: 't', 31: 'c', 32: 'u', 33: 'w', 34: 'd', 35: 'n', 36: ',', 37: 'h', 38: 'b', 39: 'y', 40: \"'\", 41: 'o', 42: 'm', 43: 'g', 44: 'v', 45: 'p', 46: 'l', 47: ':', 48: 'k', 49: 'z', 50: 'x', 51: '!', 52: ';', 53: '?', 54: 'C', 55: 'q', 56: 'j', 57: 'X'}\n",
      "tensor([ 0.,  0.,  0.,  ..., 45., 32., 45.], dtype=torch.float64)\n",
      "    SONNETS\n",
      "\n",
      "\n",
      "\n",
      "TO THE ONLY BEGETTER OF\n",
      "THESE INSUING SONNETS\n",
      "MR. W. H. ALL HAPPINESS\n",
      "AND THAT ETERNITY\n",
      "PROMISED BY\n",
      "OUR EVER-LIVING POET WISHETH\n",
      "THE WELL-WISHING\n",
      "ADVENTURER IN\n",
      "SETTING FORTH\n",
      "T. T.\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "FROM fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light'st flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel.\n",
      "Thou that art now the world's fresh ornament\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content\n",
      "And, tender churl, makest waste in niggarding.\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world's due, by the grave and thee.\n",
      "\n",
      "II.\n",
      "\n",
      "When forty winters shall beseige thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery, so gazed on now,\n",
      "Will be a tatter'd weed, of small worth held:\n",
      "Then being ask'd where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days,\n",
      "To say, within thine own deep-sunken eyes,\n",
      "Were an all-eating shame and thriftless praise.\n",
      "How much more praise deserved thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count and make my old excuse,'\n",
      "Proving his beauty by succession thine!\n",
      "  This were to be new made when thou art old,\n",
      "  And see thy blood warm when thou feel'st it cold.\n",
      "\n",
      "III.\n",
      "\n",
      "Look in thy glass, and tell the face thou viewest\n",
      "Now is the time that face should form another;\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose unear'd womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb\n",
      "Of his self-love, to stop posterity?\n",
      "Thou art thy mother's glass, and she in thee\n",
      "Calls back the lovely April of her prime:\n",
      "So thou through windows of thine age shall see\n",
      "Despite of wrinkles this thy golden time.\n",
      "  But if thou live, remember'd not to be,\n",
      "  Die single, and thine image dies with thee.\n",
      "\n",
      "IV.\n",
      "\n",
      "Unthrifty loveliness, why dost thou spend\n",
      "Upon thyself thy beauty's legacy?\n",
      "Nature's bequest gives nothing but doth lend,\n",
      "And being frank she lends to those are free.\n",
      "Then, beauteous niggard, why dost thou abuse\n",
      "The bounteous largess given thee to give?\n",
      "Profitless usurer, why dost thou use\n",
      "So great a sum of sums, yet canst not live?\n",
      "For having traffic with thyself alone,\n",
      "Thou of thyself thy sweet self dost deceive.\n",
      "Then how, when nature calls thee to be gone,\n",
      "What acceptable audit canst thou leave?\n",
      "  Thy unused beauty must be tomb'd with thee,\n",
      "  Which, used, lives th' executor to be.\n",
      "\n",
      "V.\n",
      "\n",
      "Those hours, that with gentle work did frame\n",
      "The lovely gaze where every eye doth dwell,\n",
      "Will play the tyrants to the very same\n",
      "And that unfair which fairly doth excel:\n",
      "For never-resting time leads summer on\n",
      "To hideous winter and confounds him there;\n",
      "Sap cheque'd with frost and lusty leaves quite gone,\n",
      "Beauty o'ersnow'd and bareness every where:\n",
      "Then, were not summer's distillation left,\n",
      "A liquid prisoner pent in walls of glass,\n",
      "Beauty's effect with beauty were bereft,\n",
      "Nor it nor no remembrance what it was:\n",
      "  But flowers distill'd though they with winter meet,\n",
      "  Leese but their show; their substance still lives sweet.\n",
      "\n",
      "VI.\n",
      "\n",
      "Then let not winter's ragged hand deface\n",
      "In thee thy summer, ere thou be distill'd:\n",
      "Make sweet some vial; treasure thou some place\n",
      "With beauty's treasure, ere it be self-kill'd.\n",
      "That use is not forbidden usury,\n",
      "Which happies those that pay the willing loan;\n",
      "That's for thyself to breed another thee,\n",
      "Or ten times happier, be it ten for one;\n",
      "Ten times thyself were happier than thou art,\n",
      "If ten of thine ten times refigured thee:\n",
      "Then what could death do, if thou shouldst depart,\n",
      "Leaving thee living in posterity?\n",
      "  Be not self-will'd, for thou art much too fair\n",
      "  To be death's conquest and make worms thine heir.\n",
      "\n",
      "VII.\n",
      "\n",
      "Lo! in the orient when the gracious light\n",
      "Lifts up his burning head, each under eye\n",
      "Doth homage to his new-appearing sight,\n",
      "Serving with looks his sacred majesty;\n",
      "And having climb'd the steep-up heavenly hill,\n",
      "Resembling strong youth in his middle age,\n",
      "yet mortal looks adore his beauty still,\n",
      "Attending on his golden pilgrimage;\n",
      "But when from highmost pitch, with weary car,\n",
      "Like feeble age, he reeleth from the day,\n",
      "The eyes, 'fore duteous, now converted are\n",
      "From his low tract and look another way:\n",
      "  So thou, thyself out-going in thy noon,\n",
      "  Unlook'd on diest, unless thou get a son.\n",
      "\n",
      "VIII.\n",
      "\n",
      "Music to hear, why hear'st thou music sadly?\n",
      "Sweets with sweets war not, joy delights in joy.\n",
      "Why lovest thou that which thou receivest not gladly,\n",
      "Or else receivest with pleasure thine annoy?\n",
      "If the true concord of well-tuned sounds,\n",
      "By unions married, do offend thine ear,\n",
      "They do but sweetly chide thee, who confounds\n",
      "In singleness the parts that thou shouldst bear.\n",
      "Mark how one string, sweet husband to another,\n",
      "Strikes each in each by mutual ordering,\n",
      "Resembling sire and child and happy mother\n",
      "Who all in one, one pleasing note do sing:\n",
      "  Whose speechless song, being many, seeming one,\n",
      "  Sings this to thee: 'thou single wilt prove none.'\n",
      "\n",
      "IX.\n",
      "\n",
      "Is it for fear to wet a widow's eye\n",
      "That thou consumest thyself in single life?\n",
      "Ah! if thou issueless shalt hap to die.\n",
      "The world will wail thee, like a makeless wife;\n",
      "The world will be thy widow and still weep\n",
      "That thou no form of thee hast left behind,\n",
      "When every private widow well may keep\n",
      "By children's eyes her husband's shape in mind.\n",
      "Look, what an unthrift in the world doth spend\n",
      "Shifts but his place, for still the world enjoys it;\n",
      "But beauty's waste hath in the world an end,\n",
      "And kept unused, the user so destroys it.\n",
      "  No love toward others in that bosom sits\n",
      "  That on himself such murderous shame commits.\n",
      "\n",
      "X.\n",
      "\n",
      "For shame! deny that thou bear'st love to any,\n",
      "Who for thyself art so unprovident.\n",
      "Grant, if thou wilt, thou art beloved of many,\n",
      "But that thou none lovest is most evident;\n",
      "For thou art so possess'd with murderous hate\n",
      "That 'gainst thyself thou stick'st not to conspire.\n",
      "Seeking that beauteous roof to ruinate\n",
      "Which to repair should be thy chief desire.\n",
      "O, change thy thought, that I may change my mind!\n",
      "Shall hate be fairer lodged than gentle love?\n",
      "Be, as thy presence is, gracious and kind,\n",
      "Or to thyself at least kind-hearted prove:\n",
      "  Make thee another self, for love of me,\n",
      "  That beauty still may live in thine or thee.\n",
      "\n",
      "XI.\n",
      "\n",
      "As fast as thou shalt wane, so fast thou growest\n",
      "In one of thine, from that which thou departest;\n",
      "And that fresh blood which youngly thou bestowest\n",
      "Thou mayst call thine when thou from youth convertest.\n",
      "Herein lives wisdom, beauty and increase:\n",
      "Without this, folly, age and cold decay:\n",
      "If all were minded so, the times should cease\n",
      "And threescore year would make the world away.\n",
      "Let those whom Nature hath not made for store,\n",
      "Harsh featureless and rude, barrenly perish:\n",
      "Look, whom she best endow'd she gave the more;\n",
      "Which bounteous gift thou shouldst in bounty cherish:\n",
      "  She carved thee for her seal, and meant thereby\n",
      "  Thou shouldst print more, not let that copy die.\n",
      "\n",
      "XII.\n",
      "\n",
      "When I do count the clock that tells the time,\n",
      "And see the brave day sunk in hideous night;\n",
      "When I behold the violet past prime,\n",
      "And sable curls all silver'd o'er with white;\n",
      "When lofty trees I see barren of leaves\n",
      "Which erst from heat did canopy the herd,\n",
      "And summer's green all girded up in sheaves\n",
      "Borne on the bier with white and bristly beard,\n",
      "Then of thy beauty do I question make,\n",
      "That thou among the wastes of time must go,\n",
      "Since sweets and beauties do themselves forsake\n",
      "And die as fast as they see others grow;\n",
      "  And nothing 'gainst Time's scythe can make defence\n",
      "  Save breed, to brave him when he takes thee hence.\n",
      "\n",
      "XIII.\n",
      "\n",
      "O, that you were yourself! but, love, you are\n",
      "No longer yours than you yourself here live:\n",
      "Against this coming end you should prepare,\n",
      "And your sweet semblance to some other give.\n",
      "So should that beauty which you hold in lease\n",
      "Find no determination: then you were\n",
      "Yourself again after yourself's decease,\n",
      "When your sweet issue your sweet form should bear.\n",
      "Who lets so fair a house fall to decay,\n",
      "Which husbandry in honour might uphold\n",
      "Against the stormy gusts of winter's day\n",
      "And barren rage of death's eternal cold?\n",
      "  O, none but unthrifts! Dear my love, you know\n",
      "  You had a father: let your son say so.\n",
      "\n",
      "XIV.\n",
      "\n",
      "Not from the stars do I my judgment pluck;\n",
      "And yet methinks I have astronomy,\n",
      "But not to tell of good or evil luck,\n",
      "Of plagues, of dearths, or seasons' quality;\n",
      "Nor can I fortune to brief minutes tell,\n",
      "Pointing to each his thunder, rain and wind,\n",
      "Or say with princes if it shall go well,\n",
      "By oft predict that I in heaven find:\n",
      "But from thine eyes my knowledge I derive,\n",
      "And, constant stars, in them I read such art\n",
      "As truth and beauty shall together thrive,\n",
      "If from thyself to store thou wouldst convert;\n",
      "  Or else of thee this I prognosticate:\n",
      "  Thy end is truth's and beauty's doom and date.\n",
      "\n",
      "XV.\n",
      "\n",
      "When I consider every thing that grows\n",
      "Holds in perfection but a little moment,\n",
      "That this huge stage presenteth nought but shows\n",
      "Whereon the stars in secret influence comment;\n",
      "When I perceive that men as plants increase,\n",
      "Cheered and cheque'd even by the self-same sky,\n",
      "Vaunt in their youthful sap, at height decrease,\n",
      "And wear their brave state out of memory;\n",
      "Then the conceit of this inconstant stay\n",
      "Sets you most rich in youth before my sight,\n",
      "Where wasteful Time debateth with Decay,\n",
      "To change your day of youth to sullied night;\n",
      "  And all in war with Time for love of you,\n",
      "  As he takes from you, I engraft you new.\n",
      "\n",
      "XVI.\n",
      "\n",
      "But wherefore do not you a mightier way\n",
      "Make war upon this bloody tyrant, Time?\n",
      "And fortify yourself in your decay\n",
      "With means more blessed than my barren rhyme?\n",
      "Now stand you on the top of happy hours,\n",
      "And many maiden gardens yet unset\n",
      "With virtuous wish would bear your living flowers,\n",
      "Much liker than your painted counterfeit:\n",
      "So should the lines of life that life repair,\n",
      "Which this, Time's pencil, or my pup\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary and the two mapping dictionaries\n",
    "def create_vocab(text):\n",
    "    vocab, ctoi, itoc = [], {}, {} \n",
    "    for character in text :\n",
    "        if character not in vocab :\n",
    "            vocab += [character]\n",
    "            ctoi[character] = vocab.index(character)\n",
    "            itoc[vocab.index(character)] = character\n",
    "    return vocab, ctoi, itoc\n",
    "\n",
    "\n",
    "# Implement the function converting text to tensor\n",
    "def text_to_tensor(text, ctoi):\n",
    "    arr = np.zeros((len(text)))\n",
    "    for idx, char in enumerate(text):\n",
    "        arr[idx] = ctoi[char] \n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "\n",
    "\n",
    "# Implement the function converting tensor to text\n",
    "def tensor_to_text(tensor, itoc):\n",
    "    arr = tensor.detach().numpy()\n",
    "    text = \"\"\n",
    "    for elm in arr :\n",
    "        text+=itoc[elm]\n",
    "    return text\n",
    "\n",
    "# Apply your functions to some text data\n",
    "\n",
    "vocab, ctoi, itoc = create_vocab(text_data)\n",
    "\n",
    "print(vocab)\n",
    "print(ctoi)\n",
    "print(itoc)\n",
    "\n",
    "example_tensor = text_to_tensor(text_data, ctoi)\n",
    "print(example_tensor)\n",
    "\n",
    "example_text = tensor_to_text(example_tensor, itoc)\n",
    "print(example_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfd5a4df6e6081581ad38d7180fddfb",
     "grade": false,
     "grade_id": "cell-172b8befc4633227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Setup a character-level recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb30929efdd10eb6066750f4b94bf14",
     "grade": false,
     "grade_id": "embedding",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609747a0-6634-4232-92cb-72946ef516b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9dd84493589d73c13fac37411610a2e",
     "grade": true,
     "grade_id": "cell-a3b5adeb8111779b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba287efa3849a2ddcb98b94b287fd199",
     "grade": true,
     "grade_id": "cell-4065672821a5801f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6897eae892e4aa50cceac9bca0a83f82",
     "grade": false,
     "grade_id": "base-rnn",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dd7af-cf96-4279-b512-1507433c7826",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095b1c904c7c5fa3d5371de208f7300a",
     "grade": true,
     "grade_id": "cell-c0b3cdd14603b6d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56780e84-8767-4941-ba0a-2047b106fc35",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f3230528c7fabd5b26931934ac4e8b0",
     "grade": true,
     "grade_id": "cell-9c6c3e3359e2c37e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3993b8b02d0be0e16de189c4850bbfce",
     "grade": false,
     "grade_id": "rnn-model",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Create a simple RNN model with a custom `nn.Module` class. It should contain: an embedding layer, a single-layer RNN, and a dense output layer. For each character of the input sequence, the model should predict the probability of the next character. The forward method should return the probabilities for next characters and the corresponding hidden states.\n",
    "After completing the class, create a model and apply the forward pass on some input text. Understand and explain the results.\n",
    "\n",
    "*Note:* depending on how you implement the loss function later, it can be convenient to return logits instead of probabilities, i.e. raw values of the output layer before any activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b587c6f505f5d3719869ae8a57fcb5c6",
     "grade": true,
     "grade_id": "cell-e55d41dd89bcf74f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        '''Initialize model parameters and layers.'''\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, tensor_data, hidden_state=None):\n",
    "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and apply the forward pass on some input text\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb09a2-2c5e-4dd6-922e-a7f0c6cbff85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae8f4a875df816da1463fc833e40cea4",
     "grade": true,
     "grade_id": "cell-093600fc493e6e4f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41614741-8bf2-4ead-8205-788623404a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb6ca5b49b7c8b46253cf2a7f1200c5",
     "grade": false,
     "grade_id": "rnn-overfit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa15210da5aabdca4ecf3228efec88be",
     "grade": true,
     "grade_id": "cell-1904f4989149b1ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
    "def train_overfit(model, input_seq, target_seq, n_iters=200, learning_rate=0.2):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and make it overfit the input sequence\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd6d0a90166cea96ce250f8d8f2e083",
     "grade": false,
     "grade_id": "rnn-argmax",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0114ed939e7dd8b656bd0f03bd576a89",
     "grade": true,
     "grade_id": "cell-2d706c010aeccb0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_argmax(self, context_tensor, n_predictions):\n",
    "        # Apply the forward pass for the context tensor\n",
    "        # Then, store the last prediction and last hidden state\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
    "        # Do this in a loop to predict the next `n_predictions` characters\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and make it overfit as above\n",
    "# Then, verify your overfitting by predicting characters given some context\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bd5ff990e4cf55e89b26541d0acf34",
     "grade": true,
     "grade_id": "cell-b783299fd35282d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f34d247477f051d257bc1337cfc611fa",
     "grade": false,
     "grade_id": "cell-52baebc1e4eb464c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cefa534edd726def1328ea0b48ed29d",
     "grade": false,
     "grade_id": "cell-e85a5e3954f17ad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba3d32edc8f535eb8923fb8e71c9fe4",
     "grade": true,
     "grade_id": "rnn-sample",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_proba(self, input_context, n_predictions):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Verify that your predictions are not deterministic anymore\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56bfe8d33a343e270cfe35720aeea26",
     "grade": false,
     "grade_id": "cell-6389d46b2b8abaa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Train the RNN model on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f87a393c4ae266b59141953d170a7e",
     "grade": false,
     "grade_id": "rnn-train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17faecb7751e77b7fe8cae66820687ea",
     "grade": true,
     "grade_id": "cell-a4695fabcf78e1a8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the text dataset, compute its mappings and convert it to tensor\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize training parameters\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize a character-level RNN model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Setup the training loop\n",
    "# Regularly record the loss and sample from the model to monitor what is happening\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113032562a0b82e504d636abf3164360",
     "grade": false,
     "grade_id": "rnn-predict",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d516b1-477c-4237-956a-471062dd6019",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd724d3a1f3ba5d0b965b9cf7905160",
     "grade": true,
     "grade_id": "cell-08bfe03b817a9908",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288f4746f3c7d1906ca99b35f4a6a6e3",
     "grade": true,
     "grade_id": "cell-6b41d47e15ea128a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eddbf9984d6a4d51a7ea1301800bdf3",
     "grade": false,
     "grade_id": "cell-a69a65798f792cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Experiment with different RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977d70928beb993f5bc6323199b1e363",
     "grade": false,
     "grade_id": "rnn-experiments",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Experiment with different RNN architecures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ebb731623feb1292e7758788ad56d4",
     "grade": true,
     "grade_id": "cell-7bbcfb8355f44b5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13329eaa-6a48-47cf-912d-424a79680f91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bffac2e6bdfed739aea204b3c40a792",
     "grade": true,
     "grade_id": "cell-3961b7f97f038a4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
