{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1502bbeee981ecc3a7a4464df92be0c",
     "grade": false,
     "grade_id": "cell-a975d7bfb06f30ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP3 - Recurrent neural networks\n",
    "\n",
    "Credits: Andrej Karpathy\n",
    "\n",
    "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available() # For macOS\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing\n",
    "\n",
    "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `shakespeare.txt` contains 95665 characters.\n",
      "Excerpt of the dataset:\n",
      "    SONNETS\n",
      "\n",
      "\n",
      "\n",
      "TO THE ONLY BEGETTER OF\n",
      "THESE INSUING SONNETS\n",
      "MR. W. H. ALL HAPPINESS\n",
      "AND THAT ETERNITY\n",
      "PROMISED BY\n",
      "OUR EVER-LIVING POET WISHETH\n",
      "THE WELL-WISHING\n",
      "ADVENTURER IN\n",
      "SETTING FORTH\n",
      "T. T.\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "FROM fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine own bright eyes,\n",
      "Feed'st thy light'st flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thyself thy foe, to thy sweet self too cruel.\n",
      "Thou that art now the world's fresh ornament\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content\n",
      "And, tender churl, makest waste in niggarding.\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world's due, by the grave and thee.\n",
      "\n",
      "II.\n",
      "\n",
      "When forty winters shall beseige thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery, so gazed on now,\n",
      "Will be a tatter'd weed, of small worth held:\n",
      "Then being ask'd where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days,\n",
      "To say, within thine own deep-sunken eyes,\n",
      "Were an all-eating shame and thriftless praise.\n",
      "How much more praise deserved thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count and make my old excuse,'\n",
      "Proving his beauty by succession thine!\n",
      "  This were to be new made when thou art old,\n",
      "  And see thy blood warm when thou feel'st it cold.\n",
      "\n",
      "III.\n",
      "\n",
      "Look in thy glass, and tell the face thou viewest\n",
      "Now is the time that face should form another;\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose unear'd womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb\n",
      "Of his self-love, to stop posterity?\n",
      "Thou art thy mother's glass, and she in thee\n",
      "Calls back the lovely April of her prime:\n",
      "So thou through windows of thine age shall see\n",
      "Despite of wrinkles this thy golden time.\n",
      "  But if thou\n"
     ]
    }
   ],
   "source": [
    "# text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
    "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
    "text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
    "# text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
    "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
    "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
    "\n",
    "text_data = open(text_data_fname, 'r').read()\n",
    "text_data = text_data\n",
    "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
    "print('Excerpt of the dataset:')\n",
    "print(text_data[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b030743489f86ece30d79835434297",
     "grade": false,
     "grade_id": "cell-9695c6f2cf95337c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "885ecc8a-c654-463b-9353-f7a18a607199",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3eb46710b4006993f4eb9c557a2376f",
     "grade": true,
     "grade_id": "vocabulary",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'S', 'O', 'N', 'E', 'T', '\\n', 'H', 'L', 'Y', 'B', 'G', 'R', 'F', 'I', 'U', 'M', '.', 'W', 'A', 'P', 'D', 'V', '-', 'f', 'a', 'i', 'r', 'e', 's', 't', 'c', 'u', 'w', 'd', 'n', ',', 'h', 'b', 'y', \"'\", 'o', 'm', 'g', 'v', 'p', 'l', ':', 'k', 'z', 'x', '!', ';', '?', 'C', 'q', 'j', 'X', 'K', 'J', '[', ']']\n",
      "{' ': 0, 'S': 1, 'O': 2, 'N': 3, 'E': 4, 'T': 5, '\\n': 6, 'H': 7, 'L': 8, 'Y': 9, 'B': 10, 'G': 11, 'R': 12, 'F': 13, 'I': 14, 'U': 15, 'M': 16, '.': 17, 'W': 18, 'A': 19, 'P': 20, 'D': 21, 'V': 22, '-': 23, 'f': 24, 'a': 25, 'i': 26, 'r': 27, 'e': 28, 's': 29, 't': 30, 'c': 31, 'u': 32, 'w': 33, 'd': 34, 'n': 35, ',': 36, 'h': 37, 'b': 38, 'y': 39, \"'\": 40, 'o': 41, 'm': 42, 'g': 43, 'v': 44, 'p': 45, 'l': 46, ':': 47, 'k': 48, 'z': 49, 'x': 50, '!': 51, ';': 52, '?': 53, 'C': 54, 'q': 55, 'j': 56, 'X': 57, 'K': 58, 'J': 59, '[': 60, ']': 61}\n",
      "{0: ' ', 1: 'S', 2: 'O', 3: 'N', 4: 'E', 5: 'T', 6: '\\n', 7: 'H', 8: 'L', 9: 'Y', 10: 'B', 11: 'G', 12: 'R', 13: 'F', 14: 'I', 15: 'U', 16: 'M', 17: '.', 18: 'W', 19: 'A', 20: 'P', 21: 'D', 22: 'V', 23: '-', 24: 'f', 25: 'a', 26: 'i', 27: 'r', 28: 'e', 29: 's', 30: 't', 31: 'c', 32: 'u', 33: 'w', 34: 'd', 35: 'n', 36: ',', 37: 'h', 38: 'b', 39: 'y', 40: \"'\", 41: 'o', 42: 'm', 43: 'g', 44: 'v', 45: 'p', 46: 'l', 47: ':', 48: 'k', 49: 'z', 50: 'x', 51: '!', 52: ';', 53: '?', 54: 'C', 55: 'q', 56: 'j', 57: 'X', 58: 'K', 59: 'J', 60: '[', 61: ']'}\n",
      "tensor([ 0,  0,  0,  ..., 44, 28, 17])\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary and the two mapping dictionaries\n",
    "def create_vocab(text):\n",
    "    vocab, ctoi, itoc = [], {}, {}\n",
    "    for character in text :\n",
    "        if character not in vocab :\n",
    "            vocab += [character]\n",
    "            ctoi[character] = vocab.index(character)\n",
    "            itoc[vocab.index(character)] = character\n",
    "    return vocab, ctoi, itoc\n",
    "\n",
    "# Implement the function converting text to tensor\n",
    "def text_to_tensor(text, ctoi):\n",
    "    arr = np.zeros((len(text)))\n",
    "    for idx, char in enumerate(text):\n",
    "        arr[idx] = ctoi[char]\n",
    "    return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "# Implement the function converting tensor to text\n",
    "def tensor_to_text(tensor, itoc):\n",
    "    arr = tensor.cpu().detach().numpy()\n",
    "    text = \"\"\n",
    "    for elm in arr :\n",
    "        text+=itoc[elm]\n",
    "    return text\n",
    "\n",
    "# Apply your functions to some text data\n",
    "vocab, ctoi, itoc = create_vocab(text_data)\n",
    "\n",
    "print(vocab)\n",
    "print(ctoi)\n",
    "print(itoc)\n",
    "\n",
    "example_tensor = text_to_tensor(text_data, ctoi)\n",
    "print(example_tensor)\n",
    "\n",
    "example_text = tensor_to_text(example_tensor, itoc)\n",
    "# verify integrity\n",
    "assert example_text == text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfd5a4df6e6081581ad38d7180fddfb",
     "grade": false,
     "grade_id": "cell-172b8befc4633227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Setup a character-level recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb30929efdd10eb6066750f4b94bf14",
     "grade": false,
     "grade_id": "embedding",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "609747a0-6634-4232-92cb-72946ef516b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9dd84493589d73c13fac37411610a2e",
     "grade": true,
     "grade_id": "cell-a3b5adeb8111779b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1161e+00, -3.4858e-01, -2.1906e-01,  ..., -1.4249e+00,\n",
      "         -8.6395e-01, -1.1241e+00],\n",
      "        [-2.1161e+00, -3.4858e-01, -2.1906e-01,  ..., -1.4249e+00,\n",
      "         -8.6395e-01, -1.1241e+00],\n",
      "        [-2.1161e+00, -3.4858e-01, -2.1906e-01,  ..., -1.4249e+00,\n",
      "         -8.6395e-01, -1.1241e+00],\n",
      "        ...,\n",
      "        [ 1.5299e+00,  2.0807e+00,  1.0633e+00,  ..., -8.4153e-01,\n",
      "          7.8899e-01, -7.3651e-01],\n",
      "        [-6.8824e-01,  8.8730e-01, -1.9849e-02,  ...,  2.2095e+00,\n",
      "          2.9298e-01,  4.8591e-01],\n",
      "        [ 4.1772e-01, -5.1739e-01, -6.7781e-01,  ..., -7.1451e-04,\n",
      "         -1.2020e+00, -2.0617e-01]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([95665, 16])\n"
     ]
    }
   ],
   "source": [
    "# n_vocab : the total number of unique indices that the embedding layer can handle.\n",
    "# n_dim : the size of the vector space in which the indices will be embedded.\n",
    "n_vocab, n_dim = len(vocab), 16\n",
    "\n",
    "# initiate the Embedding layer\n",
    "emb_layer = nn.Embedding(n_vocab, embedding_dim=n_dim)\n",
    "\n",
    "# given the example tensor generate an embedding of each index (indirectly character) of the text\n",
    "emb_data = emb_layer(example_tensor)\n",
    "\n",
    "print(emb_data)\n",
    "print(emb_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba287efa3849a2ddcb98b94b287fd199",
     "grade": true,
     "grade_id": "cell-4065672821a5801f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "\n",
    "**nn.Embedding converts integer indices to dense vectors. It initializes a tensor with size (sequence_length, embedding_dim), mapping each index to a learnable vector**\n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6897eae892e4aa50cceac9bca0a83f82",
     "grade": false,
     "grade_id": "base-rnn",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "162dd7af-cf96-4279-b512-1507433c7826",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095b1c904c7c5fa3d5371de208f7300a",
     "grade": true,
     "grade_id": "cell-c0b3cdd14603b6d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95665, 16]) tensor([[ 0.2224,  0.8510, -0.5396,  ...,  0.6880, -0.7641,  0.5719],\n",
      "        [-0.1041,  0.7006, -0.6765,  ...,  0.7326, -0.8644,  0.6854],\n",
      "        [-0.1355,  0.7607, -0.7111,  ...,  0.8552, -0.7828,  0.7309],\n",
      "        ...,\n",
      "        [ 0.7744, -0.2510, -0.1579,  ...,  0.7546,  0.5077,  0.4242],\n",
      "        [-0.4397,  0.2145,  0.6670,  ...,  0.3338, -0.4755,  0.1775],\n",
      "        [ 0.2874, -0.7300, -0.3092,  ...,  0.4056,  0.7646, -0.3804]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2874, -0.7300, -0.3092,  0.6968, -0.0470,  0.4345,  0.5894,  0.1318,\n",
      "         -0.2219, -0.7425,  0.5248,  0.2308,  0.5748,  0.4056,  0.7646, -0.3804]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size = 16, 16\n",
    "rnn_layer = nn.RNN(input_size, hidden_size)\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden_state = torch.zeros(1, hidden_size)\n",
    "\n",
    "# run the embedded data through the RNN\n",
    "output_sequence, final_hidden_state = rnn_layer(emb_data, hidden_state)\n",
    "\n",
    "print(output_sequence.shape, output_sequence)\n",
    "print(final_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56780e84-8767-4941-ba0a-2047b106fc35",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f3230528c7fabd5b26931934ac4e8b0",
     "grade": true,
     "grade_id": "cell-9c6c3e3359e2c37e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "The `output_sequence` represents the output of the RNN at each time step when it processes an input sequence. It is a tensor that contains the predicted value for each hidden state at each time step in the sequence. And the `final_hidden_state` is, as shown, the final hidden state of the layer.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3993b8b02d0be0e16de189c4850bbfce",
     "grade": false,
     "grade_id": "rnn-model",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Create a simple RNN model with a custom `nn.Module` class. It should contain: an embedding layer, a single-layer RNN, and a dense output layer. For each character of the input sequence, the model should predict the probability of the next character. The forward method should return the probabilities for next characters and the corresponding hidden states.\n",
    "After completing the class, create a model and apply the forward pass on some input text. Understand and explain the results.\n",
    "\n",
    "*Note:* depending on how you implement the loss function later, it can be convenient to return logits instead of probabilities, i.e. raw values of the output layer before any activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b587c6f505f5d3719869ae8a57fcb5c6",
     "grade": true,
     "grade_id": "cell-e55d41dd89bcf74f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5.7090e-02,  1.4130e-01, -2.7209e-02,  2.2851e-01, -9.3183e-02,\n",
       "           7.5359e-03, -1.2029e-01,  7.8917e-02, -2.2412e-01, -6.9306e-02,\n",
       "          -3.4203e-01, -1.1430e-01,  2.8936e-02,  3.1206e-02, -1.8886e-03,\n",
       "           1.3368e-01,  3.1732e-03,  1.4948e-01, -2.3957e-01,  6.8072e-02,\n",
       "          -2.4480e-01, -7.5529e-03,  3.8474e-01,  1.1661e-01,  9.1674e-02,\n",
       "           8.3646e-02, -4.1155e-02, -2.7405e-02,  1.6148e-01, -1.1493e-01,\n",
       "          -2.4704e-02, -1.3979e-01,  2.3415e-03,  8.9720e-02,  3.3569e-01,\n",
       "          -2.5884e-01,  2.1455e-01, -1.4433e-02, -1.1946e-01,  1.3713e-01,\n",
       "           2.8650e-02,  4.1041e-02, -1.3497e-02, -1.1016e-01, -1.4553e-01,\n",
       "          -6.7254e-02, -2.5757e-01,  1.6758e-02,  1.5810e-01, -3.3543e-03,\n",
       "          -1.0840e-01,  1.9467e-01,  1.2757e-01,  8.9506e-02,  8.6306e-02,\n",
       "          -5.8471e-03,  3.7965e-02,  1.4488e-02, -2.1332e-01, -3.1487e-01,\n",
       "           2.5038e-01, -2.5073e-01],\n",
       "         [ 2.7795e-02,  3.0029e-01, -8.8868e-02,  1.2846e-01, -7.9250e-02,\n",
       "          -5.2524e-02, -5.7753e-02,  2.7122e-01, -9.9635e-02, -7.2151e-02,\n",
       "          -4.0043e-01, -2.4183e-02,  8.7302e-02, -8.5243e-03, -1.2650e-02,\n",
       "           1.3746e-01,  1.8867e-02,  9.1868e-02, -2.6508e-01,  9.8085e-02,\n",
       "          -1.5913e-01,  1.1527e-01,  3.0311e-01,  1.9483e-01,  1.2694e-01,\n",
       "           9.3974e-02, -2.0722e-02,  7.6855e-02,  9.7979e-02,  1.0989e-02,\n",
       "           8.5011e-02, -5.9896e-02,  1.3883e-01,  6.9064e-02,  3.1298e-01,\n",
       "          -2.2778e-01,  1.2853e-01, -4.9474e-02, -1.7774e-01,  1.9918e-01,\n",
       "          -1.2454e-02, -3.1955e-02,  1.1334e-02, -1.1732e-01, -2.6116e-01,\n",
       "          -3.5113e-02, -2.5221e-01,  5.9294e-02,  7.2637e-02, -1.5602e-02,\n",
       "          -2.0255e-01,  1.6475e-01,  4.0352e-02,  8.7923e-02,  7.1164e-03,\n",
       "          -1.0055e-01,  7.3468e-02,  4.2805e-02, -1.8613e-01, -3.1239e-01,\n",
       "           3.8268e-01, -3.3889e-01],\n",
       "         [ 1.0667e-02,  3.0722e-01, -8.6237e-02,  2.3335e-01, -7.0983e-02,\n",
       "          -5.6591e-02, -5.4196e-02,  3.1343e-01, -2.1571e-01, -2.9824e-02,\n",
       "          -3.3717e-01,  6.7198e-02,  3.4870e-02, -6.4476e-02, -4.2500e-02,\n",
       "           1.1504e-01,  8.9758e-02,  1.0042e-01, -1.7935e-01,  6.1935e-02,\n",
       "          -1.4065e-01,  5.9055e-02,  2.9509e-01,  2.3193e-01,  1.8364e-01,\n",
       "           1.0963e-01, -4.6838e-02,  9.0585e-02,  5.8485e-02,  1.1427e-02,\n",
       "           9.1161e-02, -1.1633e-01,  1.9676e-01,  7.8201e-02,  2.5700e-01,\n",
       "          -2.0016e-01,  1.3678e-01, -5.6209e-02, -1.7871e-01,  2.3929e-01,\n",
       "           2.0897e-02, -1.3005e-02,  7.9279e-03, -7.6511e-02, -3.0187e-01,\n",
       "           2.4684e-03, -1.8730e-01,  9.0545e-02,  7.3095e-02, -2.3839e-02,\n",
       "          -1.8321e-01,  9.4060e-02,  4.0691e-02,  5.0660e-02, -3.7133e-03,\n",
       "          -5.0545e-02,  4.8856e-02,  4.8327e-02, -1.5720e-01, -2.5610e-01,\n",
       "           4.0831e-01, -3.1330e-01],\n",
       "         [ 6.9716e-03,  2.9789e-01, -9.8800e-02,  2.8979e-01, -5.2583e-02,\n",
       "          -7.6907e-03, -6.4572e-02,  2.8147e-01, -2.0384e-01, -4.0708e-02,\n",
       "          -3.2573e-01,  9.2429e-02,  6.9082e-02, -5.3998e-02, -2.0956e-02,\n",
       "           9.9224e-02,  1.4728e-01,  6.2378e-02, -2.1057e-01,  8.0498e-02,\n",
       "          -1.5607e-01,  4.0961e-02,  3.2756e-01,  2.6140e-01,  1.5363e-01,\n",
       "           1.5141e-01, -3.5681e-02,  6.8651e-02,  4.0971e-02, -9.5771e-03,\n",
       "           6.2540e-02, -1.0772e-01,  2.1990e-01,  9.2956e-02,  2.9407e-01,\n",
       "          -1.7074e-01,  1.2832e-01, -3.6913e-02, -1.9332e-01,  2.5220e-01,\n",
       "           8.8616e-03, -6.1653e-03,  3.8764e-02, -9.4888e-02, -2.4595e-01,\n",
       "           3.9536e-02, -2.0696e-01,  8.3403e-02,  7.4268e-02,  7.3035e-04,\n",
       "          -1.6431e-01,  1.0040e-01,  1.4271e-02,  3.4434e-02, -2.0809e-02,\n",
       "          -8.1745e-02,  1.1549e-01,  7.8600e-02, -1.3998e-01, -2.5639e-01,\n",
       "           3.8140e-01, -2.7569e-01],\n",
       "         [ 7.7578e-02,  4.3076e-01, -3.8476e-01,  5.0631e-01,  1.4230e-01,\n",
       "           3.7201e-01,  2.2883e-01, -7.8033e-03, -3.4152e-01, -1.5924e-01,\n",
       "           3.6661e-01,  3.6271e-01,  3.4942e-03, -2.2063e-01, -1.7090e-01,\n",
       "          -2.5812e-02,  1.8872e-01,  1.1311e-02,  2.1380e-02,  1.9258e-01,\n",
       "           2.4681e-01,  1.8641e-02,  5.1976e-02,  3.6765e-01, -1.8239e-02,\n",
       "           2.3348e-01,  1.2678e-01,  1.2226e-01, -2.7959e-02,  6.8973e-02,\n",
       "           1.3864e-01, -1.5456e-01,  5.5837e-01, -1.8590e-02, -5.8926e-02,\n",
       "           9.9665e-02,  2.1428e-01, -2.9592e-01, -1.1830e-01, -8.3052e-02,\n",
       "          -7.2710e-02, -3.7208e-02, -5.0839e-02, -7.8751e-02,  6.6867e-01,\n",
       "          -8.9310e-02, -1.7545e-01,  2.9918e-01, -2.0464e-01, -1.0166e-01,\n",
       "           1.7745e-02, -1.9206e-01, -8.6476e-02, -1.2105e-03,  6.6648e-02,\n",
       "           6.8320e-02,  2.4517e-02, -3.4769e-02, -2.3773e-01,  7.3854e-02,\n",
       "           2.2850e-01,  1.4316e-01],\n",
       "         [ 2.6660e-01,  3.1283e-02, -3.0839e-01, -4.3053e-02, -3.2328e-01,\n",
       "           3.0203e-01, -6.5179e-02,  4.8404e-01,  1.1878e-01, -1.9276e-01,\n",
       "           4.2817e-02, -1.2793e-01, -2.2902e-01,  7.8648e-02, -6.4938e-03,\n",
       "          -2.0002e-01,  3.8202e-01,  1.0173e-01, -5.2493e-03, -1.2917e-02,\n",
       "          -7.1228e-02, -3.4258e-02, -1.4634e-01, -2.1196e-01, -2.8273e-01,\n",
       "           3.7621e-01,  3.5634e-03,  7.5434e-02, -1.7250e-01, -3.2722e-01,\n",
       "          -7.3372e-02, -1.3600e-01,  5.4191e-01, -8.0520e-03, -6.3197e-02,\n",
       "           2.3083e-01, -1.9365e-02, -1.7853e-01, -8.8074e-02,  1.3295e-01,\n",
       "          -2.9526e-01, -1.7435e-01,  7.9966e-02,  3.3121e-01,  8.5028e-01,\n",
       "           1.7552e-01,  1.4544e-01,  5.5982e-02, -5.3431e-02,  6.4780e-02,\n",
       "           1.2384e-01, -3.8394e-01, -1.5551e-02,  3.0856e-01,  2.3375e-01,\n",
       "          -8.9166e-02,  3.3975e-01, -1.7890e-01,  6.7143e-01, -4.5909e-02,\n",
       "           1.5768e-01, -8.6265e-02],\n",
       "         [-2.1757e-01,  2.6478e-01, -5.9300e-01,  8.9316e-02,  2.3473e-01,\n",
       "           2.7929e-01, -2.2429e-01, -9.0209e-02,  8.3079e-02, -4.0399e-01,\n",
       "          -9.4952e-02, -3.6174e-01,  1.4755e-01,  2.7283e-01, -5.1947e-02,\n",
       "          -8.7704e-02, -1.6444e-02, -1.1834e-01, -5.4988e-01, -3.5669e-01,\n",
       "           4.3595e-01,  4.7002e-02,  1.5242e-01,  1.1814e-01, -2.2251e-01,\n",
       "          -1.3027e-01, -7.4641e-02,  1.0252e-01, -3.3803e-02, -2.6916e-01,\n",
       "           3.2560e-01,  2.0692e-02,  9.2962e-02,  2.9957e-01,  3.9704e-01,\n",
       "          -5.3005e-02,  2.4008e-01, -3.2007e-01, -1.5375e-01,  5.3802e-02,\n",
       "           2.4060e-02,  1.6871e-01,  7.5875e-02,  2.3364e-01,  3.9980e-01,\n",
       "           4.5343e-02, -2.9526e-01,  1.4721e-01,  4.8431e-02,  3.6328e-01,\n",
       "           3.9151e-01, -1.3665e-01,  2.5572e-01, -3.1407e-02,  7.6126e-01,\n",
       "          -1.6564e-01,  2.2981e-01,  3.6736e-02,  7.0123e-02, -5.7363e-02,\n",
       "           3.0317e-01,  1.5454e-01],\n",
       "         [-1.3103e-01,  2.7496e-01, -4.2451e-01, -1.9023e-01, -3.0477e-03,\n",
       "           1.1337e-01,  6.8412e-03, -8.2537e-02,  3.5411e-02, -1.7693e-01,\n",
       "          -1.7076e-01, -2.5160e-01, -8.2225e-02,  3.3111e-01, -3.7854e-01,\n",
       "          -3.5506e-02, -9.4760e-02, -1.7801e-01, -4.7590e-01, -2.8325e-01,\n",
       "           3.2112e-01,  4.3867e-02,  1.2250e-01,  9.3115e-02, -2.8517e-01,\n",
       "          -8.7071e-02, -1.9572e-01,  1.8802e-01,  7.4130e-02, -1.3330e-01,\n",
       "           1.0247e-01,  2.7701e-01, -1.1312e-02,  2.3783e-01,  1.5219e-01,\n",
       "          -2.0875e-01, -7.5938e-03, -2.1107e-01, -2.9228e-02, -3.9470e-02,\n",
       "          -6.1444e-02,  6.1341e-02,  2.0765e-02,  1.7874e-01,  3.8860e-01,\n",
       "          -1.3670e-01, -2.9392e-01,  1.0658e-01,  8.9678e-02,  1.0693e-01,\n",
       "           1.1366e-01,  7.6702e-02,  1.8588e-01,  3.3139e-02,  6.2364e-01,\n",
       "          -3.4106e-01, -1.7242e-01,  1.5863e-03,  1.0268e-01, -2.3687e-01,\n",
       "           1.0347e-01, -5.5304e-02],\n",
       "         [ 8.5290e-02, -2.6697e-01, -4.8800e-01, -2.9200e-01, -4.5309e-01,\n",
       "           1.1532e-01,  3.7274e-01, -3.1151e-01, -6.3225e-02,  1.4353e-01,\n",
       "          -8.8024e-02, -1.4750e-01, -5.5718e-01,  3.2963e-01, -1.3439e-01,\n",
       "          -1.7524e-01, -2.9789e-01,  3.3481e-01,  1.2485e-01, -2.7778e-01,\n",
       "          -3.2131e-01,  3.7322e-01, -3.5376e-03, -3.2745e-02, -2.3338e-01,\n",
       "           1.2275e-01, -1.8014e-01,  1.3089e-01,  4.5437e-01,  1.4949e-01,\n",
       "           1.8035e-01,  4.1262e-01, -7.2910e-02,  8.1770e-02, -2.9350e-01,\n",
       "          -3.1227e-01, -9.3137e-02, -5.2892e-01, -1.3753e-02, -1.9259e-01,\n",
       "          -9.1164e-02, -3.9645e-01, -3.7959e-01,  4.3493e-01,  4.6420e-01,\n",
       "          -2.3786e-01,  3.9763e-02,  6.1880e-03, -8.1952e-02, -7.0688e-02,\n",
       "          -1.5052e-01, -1.8906e-01, -4.8573e-01,  7.0747e-02,  3.2346e-01,\n",
       "          -9.7576e-02, -1.8135e-01, -8.5536e-02,  5.0137e-01, -1.6447e-01,\n",
       "           5.2302e-02, -8.3699e-02],\n",
       "         [ 9.6586e-02,  1.7649e-01, -1.4736e-01,  2.8031e-01, -1.0865e-01,\n",
       "          -2.7281e-01, -2.6840e-01,  3.0154e-01, -8.6242e-02,  1.1906e-01,\n",
       "          -1.1397e-01,  2.7921e-02,  3.7026e-02,  1.8636e-01, -2.0358e-01,\n",
       "          -1.7833e-01, -1.7811e-04,  7.2403e-02, -3.9746e-01,  1.4538e-01,\n",
       "           2.6548e-01, -4.2069e-01,  3.0734e-01, -1.4498e-01, -2.8781e-03,\n",
       "          -1.5629e-01,  3.8665e-01,  6.3015e-03, -4.8743e-01, -1.3459e-01,\n",
       "           2.5651e-02, -2.6511e-01, -8.3958e-02,  3.7051e-01,  4.1327e-01,\n",
       "          -1.4915e-01,  5.5223e-02,  3.4485e-01, -4.0207e-01,  4.4866e-01,\n",
       "          -1.1363e-01, -2.4402e-01, -2.1999e-01,  4.5495e-01,  9.9193e-02,\n",
       "           1.1511e-03, -5.0547e-01,  1.5719e-01,  1.2184e-02,  3.4858e-01,\n",
       "          -1.3398e-01,  5.0467e-01, -1.7867e-01,  2.1564e-02, -1.4351e-01,\n",
       "           6.8858e-02,  1.7024e-01, -1.3053e-01, -1.6961e-01, -9.9826e-02,\n",
       "           5.7260e-01, -4.8872e-01]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.4405, -0.4696, -0.2699, -0.0379, -0.0994, -0.5214, -0.6015, -0.6307,\n",
       "          -0.0548,  0.1273, -0.6488,  0.7119, -0.8652, -0.2538, -0.0132, -0.1754,\n",
       "           0.5529,  0.0612,  0.4669, -0.2913,  0.7061, -0.8034,  0.4832,  0.7650,\n",
       "           0.5766,  0.0772,  0.0362,  0.2463,  0.3700, -0.6874,  0.2215, -0.1446,\n",
       "          -0.8221,  0.2385, -0.0146, -0.4176,  0.4277,  0.4187, -0.4369, -0.3847,\n",
       "          -0.4128,  0.1410, -0.6177,  0.1124,  0.7711, -0.0357, -0.4074, -0.8671,\n",
       "           0.1141, -0.6638,  0.1623,  0.3041,  0.3114, -0.5763, -0.1445,  0.5543,\n",
       "          -0.4036, -0.0948,  0.3828,  0.0784, -0.4512, -0.2440, -0.5067,  0.3962]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        '''Initialize model parameters and layers.'''\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.lin = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tensor_data, hidden_state=None):\n",
    "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
    "        output = self.emb(tensor_data)\n",
    "        output, hidden = self.rnn(output, hidden_state)\n",
    "        output = self.lin(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Initialize a model and apply the forward pass on some input text\n",
    "vocab_size, embedding_dim, hidden_dim = len(vocab), 32, 64\n",
    "sr = SimpleRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "sr.forward(text_to_tensor(text_data[:10], ctoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb09a2-2c5e-4dd6-922e-a7f0c6cbff85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae8f4a875df816da1463fc833e40cea4",
     "grade": true,
     "grade_id": "cell-093600fc493e6e4f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41614741-8bf2-4ead-8205-788623404a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb6ca5b49b7c8b46253cf2a7f1200c5",
     "grade": false,
     "grade_id": "rnn-overfit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa15210da5aabdca4ecf3228efec88be",
     "grade": true,
     "grade_id": "cell-1904f4989149b1ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/1000, Loss: 0.2606762647628784\n",
      "Iteration 100/1000, Loss: 0.014782190322875977\n",
      "Iteration 150/1000, Loss: 1.5871313735260628e-05\n",
      "Iteration 200/1000, Loss: 6.0050838328606915e-06\n",
      "Iteration 250/1000, Loss: 4.440505108505022e-06\n",
      "Iteration 300/1000, Loss: 3.510689566610381e-06\n",
      "Iteration 350/1000, Loss: 2.8759081942553166e-06\n",
      "Iteration 400/1000, Loss: 2.3960960788826924e-06\n",
      "Iteration 450/1000, Loss: 2.035490979324095e-06\n",
      "Iteration 500/1000, Loss: 1.7255488273804076e-06\n",
      "Iteration 550/1000, Loss: 1.507993260929652e-06\n",
      "Iteration 600/1000, Loss: 1.332160536549054e-06\n",
      "Iteration 650/1000, Loss: 1.192090280710545e-06\n",
      "Iteration 700/1000, Loss: 1.0818221198860556e-06\n",
      "Iteration 750/1000, Loss: 9.86455120255414e-07\n",
      "Iteration 800/1000, Loss: 9.000286809168756e-07\n",
      "Iteration 850/1000, Loss: 8.344638331436727e-07\n",
      "Iteration 900/1000, Loss: 7.688989285270509e-07\n",
      "Iteration 950/1000, Loss: 7.212153150248923e-07\n",
      "Iteration 1000/1000, Loss: 6.735317015227338e-07\n"
     ]
    }
   ],
   "source": [
    "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
    "input_seq = torch.arange(0, 40).long()\n",
    "target_seq = (input_seq+1).clone() \n",
    "\n",
    "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
    "def train_overfit(model, input_seq, target_seq, n_iters=200, learning_rate=0.2):\n",
    "    optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    hidden = None\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        output, hidden = model(input_seq.unsqueeze(0), hidden)\n",
    "        hidden.detach_()\n",
    "        loss = loss_function(output.squeeze(0), target_seq)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if (i + 1)%50 == 0:\n",
    "            print(f\"Iteration {i + 1}/{n_iters}, Loss: {loss.item()}\")\n",
    "\n",
    "# Initialize a model and make it overfit the input sequence\n",
    "sr_model = SimpleRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "train_overfit(sr_model, input_seq, target_seq, n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd6d0a90166cea96ce250f8d8f2e083",
     "grade": false,
     "grade_id": "rnn-argmax",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0114ed939e7dd8b656bd0f03bd576a89",
     "grade": true,
     "grade_id": "cell-2d706c010aeccb0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/1000, Loss: 0.011529028415679932\n",
      "Iteration 100/1000, Loss: 0.00457511143758893\n",
      "Iteration 150/1000, Loss: 0.01494741439819336\n",
      "Iteration 200/1000, Loss: 2.860960421458003e-06\n",
      "Iteration 250/1000, Loss: 1.761298335622996e-06\n",
      "Iteration 300/1000, Loss: 1.320233764090517e-06\n",
      "Iteration 350/1000, Loss: 1.0579773288554861e-06\n",
      "Iteration 400/1000, Loss: 8.791652135187178e-07\n",
      "Iteration 450/1000, Loss: 7.59956947149476e-07\n",
      "Iteration 500/1000, Loss: 6.645901748925098e-07\n",
      "Iteration 550/1000, Loss: 5.93064953591238e-07\n",
      "Iteration 600/1000, Loss: 5.275001626614539e-07\n",
      "Iteration 650/1000, Loss: 4.768364192386798e-07\n",
      "Iteration 700/1000, Loss: 4.2021218860099907e-07\n",
      "Iteration 750/1000, Loss: 3.695483314913872e-07\n",
      "Iteration 800/1000, Loss: 3.4868676834776124e-07\n",
      "Iteration 850/1000, Loss: 3.2186477483264753e-07\n",
      "Iteration 900/1000, Loss: 3.099438572462532e-07\n",
      "Iteration 950/1000, Loss: 2.9206248086666164e-07\n",
      "Iteration 1000/1000, Loss: 2.771613480945234e-07\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(SimpleRNN):\n",
    "    def predict_argmax(self, context_tensor, n_predictions):\n",
    "        # Apply the forward pass for the context tensor\n",
    "        # Then, store the last prediction and last hidden state\n",
    "        predictions, hidden = [], None\n",
    "        for char_index in context_tensor:\n",
    "            output, hidden = self.forward(char_index.unsqueeze(0).unsqueeze(0), hidden_state=hidden)\n",
    "\n",
    "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
    "        # Do this in a loop to predict the next `n_predictions` characters\n",
    "        for _ in range(n_predictions):\n",
    "            output, hidden = self.forward(context_tensor[-1].unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "            predicted_index = output.squeeze(0).argmax().item()\n",
    "            predictions.append(predicted_index)\n",
    "\n",
    "            context_tensor = torch.cat((context_tensor, torch.tensor([predicted_index])))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Initialize a model and make it overfit as above\n",
    "# Then, verify your overfitting by predicting characters given some context\n",
    "model = CharRNN(vocab_size=len(vocab), embedding_dim=32, hidden_dim=64)\n",
    "train_overfit(model, input_seq, target_seq, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38d6df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 36, 37]\n"
     ]
    }
   ],
   "source": [
    "# predict 3 indices\n",
    "print(model.predict_argmax(input_seq[:35], n_predictions=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bd5ff990e4cf55e89b26541d0acf34",
     "grade": true,
     "grade_id": "cell-b783299fd35282d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f34d247477f051d257bc1337cfc611fa",
     "grade": false,
     "grade_id": "cell-52baebc1e4eb464c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cefa534edd726def1328ea0b48ed29d",
     "grade": false,
     "grade_id": "cell-e85a5e3954f17ad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba3d32edc8f535eb8923fb8e71c9fe4",
     "grade": true,
     "grade_id": "rnn-sample",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/1000, Loss: 0.19750407338142395\n",
      "Iteration 100/1000, Loss: 1.7572730939718895e-05\n",
      "Iteration 150/1000, Loss: 4.723605798062636e-06\n",
      "Iteration 200/1000, Loss: 3.54942494595889e-06\n",
      "Iteration 250/1000, Loss: 2.863984036594047e-06\n",
      "Iteration 300/1000, Loss: 2.393113845755579e-06\n",
      "Iteration 350/1000, Loss: 2.0384702565934276e-06\n",
      "Iteration 400/1000, Loss: 1.7672715557637275e-06\n",
      "Iteration 450/1000, Loss: 1.5586567769787507e-06\n",
      "Iteration 500/1000, Loss: 1.3828241662849905e-06\n",
      "Iteration 550/1000, Loss: 1.242754024133319e-06\n",
      "Iteration 600/1000, Loss: 1.1175848158018198e-06\n",
      "Iteration 650/1000, Loss: 1.0222178161711781e-06\n",
      "Iteration 700/1000, Loss: 9.328112469120242e-07\n",
      "Iteration 750/1000, Loss: 8.523451242581359e-07\n",
      "Iteration 800/1000, Loss: 7.897605200923863e-07\n",
      "Iteration 850/1000, Loss: 7.271758022397989e-07\n",
      "Iteration 900/1000, Loss: 6.824724323450937e-07\n",
      "Iteration 950/1000, Loss: 6.377691192938073e-07\n",
      "Iteration 1000/1000, Loss: 5.990261229271709e-07\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_proba(self, input_context, n_predictions):\n",
    "        predictions, hidden = [], None\n",
    "\n",
    "        for char_index in input_context:\n",
    "            output, hidden = self.forward(char_index.unsqueeze(0).unsqueeze(0), hidden)\n",
    "\n",
    "        for _ in range(n_predictions):\n",
    "            output, hidden = self.forward(input_context[-1].unsqueeze(0).unsqueeze(0), hidden)\n",
    "\n",
    "            # Use Categorical distribution to sample from the predicted probabilities\n",
    "            categorical_dist = distributions.Categorical(logits=output.squeeze(0))\n",
    "            predicted_index = categorical_dist.sample().item()\n",
    "            predictions.append(predicted_index)\n",
    "\n",
    "            # Update the context tensor with the new prediction\n",
    "            input_context = torch.cat((input_context, torch.tensor([predicted_index])))\n",
    "\n",
    "        return predictions\n",
    "# Verify that your predictions are not deterministic anymore\n",
    "model = CharRNN(vocab_size=len(vocab), embedding_dim=32, hidden_dim=64)\n",
    "train_overfit(model, input_seq, target_seq, n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56bfe8d33a343e270cfe35720aeea26",
     "grade": false,
     "grade_id": "cell-6389d46b2b8abaa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Train the RNN model on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f87a393c4ae266b59141953d170a7e",
     "grade": false,
     "grade_id": "rnn-train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17faecb7751e77b7fe8cae66820687ea",
     "grade": true,
     "grade_id": "cell-a4695fabcf78e1a8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 2.4149956703186035\n",
      "Epoch 20/100, Loss: 2.5526630878448486\n",
      "Epoch 30/100, Loss: 1.6975555419921875\n",
      "Epoch 40/100, Loss: 2.3195462226867676\n",
      "Epoch 50/100, Loss: 2.5276615619659424\n",
      "Epoch 60/100, Loss: 2.3649086952209473\n",
      "Epoch 70/100, Loss: 2.419177532196045\n",
      "Epoch 80/100, Loss: 2.059297561645508\n",
      "Epoch 90/100, Loss: 2.272216558456421\n",
      "Epoch 100/100, Loss: 1.9592567682266235\n"
     ]
    }
   ],
   "source": [
    "# Create the text dataset, compute its mappings and convert it to tensor\n",
    "data_tensor = text_to_tensor(text_data, ctoi)\n",
    "seq_len = 10\n",
    "\n",
    "# Initialize training parameters\n",
    "vocab_size, embedding_dim, hidden_dim = len(vocab), 32, 64\n",
    "\n",
    "\n",
    "# Initialize a character-level RNN model\n",
    "cr_model = CharRNN(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Setup the training loop\n",
    "# Regularly record the loss and sample from the model to monitor what is happening\n",
    "def train_loop(model, data_tensor, seq_len, n_epochs, learning_rate=5e-3):\n",
    "    model.train()\n",
    "    optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Randomly choose a starting point for each epoch\n",
    "        start_index = np.random.randint(0, data_tensor.size(0) - seq_len - 1)\n",
    "        hidden = None\n",
    "        for i in range(start_index, data_tensor.size(0) - seq_len, seq_len):\n",
    "            input_seq = data_tensor[i:i+seq_len].unsqueeze(0).to(device)\n",
    "            target_seq = data_tensor[i+1:i+seq_len+1].to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            hidden.detach_()\n",
    "\n",
    "            loss = loss_function(output.squeeze(0), target_seq)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if (epoch + 1)%10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "train_loop(cr_model, data_tensor, seq_len, n_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113032562a0b82e504d636abf3164360",
     "grade": false,
     "grade_id": "rnn-predict",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "53d516b1-477c-4237-956a-471062dd6019",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd724d3a1f3ba5d0b965b9cf7905160",
     "grade": true,
     "grade_id": "cell-08bfe03b817a9908",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kid playing in the playgrou e\n"
     ]
    }
   ],
   "source": [
    "start_text = \"A kid playing in the playgrou\"\n",
    "cr_model.eval()\n",
    "generated_text = start_text\n",
    "n_chars = 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_seq = text_to_tensor(start_text, ctoi).to(device)\n",
    "    predicted_indices = cr_model.predict_proba(input_seq, n_predictions=n_chars)\n",
    "    \n",
    "    for idx in predicted_indices :\n",
    "        if idx < len(vocab) :\n",
    "            generated_text += itoc[idx]\n",
    "        else : \n",
    "            print(idx)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288f4746f3c7d1906ca99b35f4a6a6e3",
     "grade": true,
     "grade_id": "cell-6b41d47e15ea128a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eddbf9984d6a4d51a7ea1301800bdf3",
     "grade": false,
     "grade_id": "cell-a69a65798f792cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Experiment with different RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977d70928beb993f5bc6323199b1e363",
     "grade": false,
     "grade_id": "rnn-experiments",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Experiment with different RNN architecures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ebb731623feb1292e7758788ad56d4",
     "grade": true,
     "grade_id": "cell-7bbcfb8355f44b5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 2.5696303844451904\n",
      "Epoch 20/50, Loss: 1.9924137592315674\n",
      "Epoch 30/50, Loss: 1.9971141815185547\n",
      "Epoch 40/50, Loss: 2.291541576385498\n",
      "Epoch 50/50, Loss: 1.968225121498108\n"
     ]
    }
   ],
   "source": [
    "cr_model_3 = CharRNN(vocab_size, embedding_dim, hidden_dim, num_layers=3)\n",
    "train_loop(cr_model_3, data_tensor, seq_len, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36b21804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kid playing in the playgrou b\n"
     ]
    }
   ],
   "source": [
    "start_text = \"A kid playing in the playgrou\"\n",
    "cr_model_3.eval()\n",
    "generated_text = start_text\n",
    "n_chars = 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_seq = text_to_tensor(start_text, ctoi).to(device)\n",
    "    predicted_indices = cr_model_3.predict_proba(input_seq, n_predictions=n_chars)\n",
    "    \n",
    "    for idx in predicted_indices :\n",
    "        if idx < len(vocab) :\n",
    "            generated_text += itoc[idx]\n",
    "        else : \n",
    "            print(idx)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "85cf96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        '''Initialize model parameters and layers.'''\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.lin = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tensor_data, hidden_state=None):\n",
    "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
    "        output = self.emb(tensor_data)\n",
    "        output, hidden = self.lstm(output, hidden_state)\n",
    "        output = self.lin(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def predict_proba(self, input_context, n_predictions):\n",
    "        predictions, hidden = [], None\n",
    "\n",
    "        for char_index in input_context:\n",
    "            output, hidden = self.forward(char_index.unsqueeze(0).unsqueeze(0), hidden)\n",
    "\n",
    "        for _ in range(n_predictions):\n",
    "            output, hidden = self.forward(input_context[-1].unsqueeze(0).unsqueeze(0), hidden)\n",
    "\n",
    "            # Use Categorical distribution to sample from the predicted probabilities\n",
    "            categorical_dist = distributions.Categorical(logits=output.squeeze(0))\n",
    "            predicted_index = categorical_dist.sample().item()\n",
    "            predictions.append(predicted_index)\n",
    "\n",
    "            # Update the context tensor with the new prediction\n",
    "            input_context = torch.cat((input_context, torch.tensor([predicted_index])))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def train_loop_lstm(model, data_tensor, seq_len, n_epochs, learning_rate=5e-3):\n",
    "    model.train()\n",
    "    optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Randomly choose a starting point for each epoch\n",
    "        start_index = np.random.randint(0, data_tensor.size(0) - seq_len - 1)\n",
    "        hidden = None\n",
    "        for i in range(start_index, data_tensor.size(0) - seq_len, seq_len):\n",
    "            input_seq = data_tensor[i:i+seq_len].unsqueeze(0).to(device)\n",
    "            target_seq = data_tensor[i+1:i+seq_len+1].to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "\n",
    "            loss = loss_function(output.squeeze(0), target_seq)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if (epoch + 1)%10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "75b2d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 1.2888818979263306\n",
      "Epoch 20/50, Loss: 1.8630107641220093\n",
      "Epoch 30/50, Loss: 1.9078855514526367\n",
      "Epoch 40/50, Loss: 1.7482788562774658\n",
      "Epoch 50/50, Loss: 1.1762248277664185\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n",
    "train_loop_lstm(lstm_model, data_tensor, seq_len, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f67af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A kid playing in the playgrousi\n"
     ]
    }
   ],
   "source": [
    "start_text = \"A kid playing in the playgrou\"\n",
    "lstm_model.eval()\n",
    "generated_text = start_text\n",
    "n_chars = 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_seq = text_to_tensor(start_text, ctoi).to(device)\n",
    "    predicted_indices = lstm_model.predict_proba(input_seq, n_predictions=n_chars)\n",
    "    \n",
    "    for idx in predicted_indices :\n",
    "        if idx < len(vocab) :\n",
    "            generated_text += itoc[idx]\n",
    "        else : \n",
    "            print(idx)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13329eaa-6a48-47cf-912d-424a79680f91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bffac2e6bdfed739aea204b3c40a792",
     "grade": true,
     "grade_id": "cell-3961b7f97f038a4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Answer :\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efde269",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
