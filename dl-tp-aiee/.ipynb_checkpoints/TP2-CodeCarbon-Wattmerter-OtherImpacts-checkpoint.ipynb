{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYgSCQFfK50i"
   },
   "source": [
    "# Several ways to measure carbon emissions when training and diving deeper into other impacts\n",
    "\n",
    "---\n",
    "\n",
    "Master 2 Informatique - Université de Bordeaux\n",
    "Image et Son  - Image Processing and Computer Vision\n",
    "\n",
    "---\n",
    "Authors : Lucia Bouza,  Aurélie Bugeau, Anne Vialard\n",
    "Acknowledgments : Anne-Laure Ligozat\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9SoVI2PIw_b"
   },
   "source": [
    "## Several ways to measure carbon emissions when training \n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In previous practice, you have studied the use of the **online tool Green Algorithms** (https://www.green-algorithms.org). In the first part of this practical session, you will compare this tool with two others for measuring the environmental impacts of deep learning code. You will calculate the carbon footprint produced by the training of a digit classification network using:\n",
    "\n",
    "#### Wattmeter: \n",
    "You will track real energy consumption from wattmeter measurements. Turn off the computer in cremi, plug it onto the wattmeter and restart it. Measures will be obtained by reading onto the wattmeter screen. Press the Mode button once to read values in Watt. \n",
    "\n",
    "Observe the values when not running any program and when running your jupyter-notebook.\n",
    "\n",
    "#### Software tool: Code Carbon\n",
    "Code Carbon is a Python library to measure carbon emissions. The notebook will explain how to install and use the library, as well as visualize the results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlGnyuE-kyPb"
   },
   "source": [
    "##### Wattmeter\n",
    "\n",
    "<span style=\"color:DarkRed\"> Q1. According to you, what are the advantages and disadvantages of using a wattmeter?  </span>\n",
    "\n",
    "\n",
    "<span style=\"color:DarkRed\"> Q2. Turn off every softwares you might be running on the computer (except from the jupyter-notebook). Observe the current consumption indicated by the wattmeter. </span>\n",
    "\n",
    "<span style=\"color:DarkRed\"> Q3. Run the command. Observe the approximate power consumption while running the program. What is the energy consumption? What is the dynamic consumption and static consumption</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !time python TrainingClassification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Practice with CodeCarbon\n",
    "\n",
    "[CodeCarbon](https://codecarbon.io) is a software package for Python. It estimates the amount of gas emissions produced by the execution of the code. CodeCarbon takes into account energy consumption and location to calculate the carbon footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS8SusTX7sgX"
   },
   "source": [
    "#### Installation\n",
    "\n",
    "You will work on cremi computer\n",
    "> Remark: On your personal computer, you can install the library by using pip or conda (for example `pip install codecarbon`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJcbSjfm2isT"
   },
   "source": [
    "#### Using CodeCarbon\n",
    "\n",
    "There are several ways of using the library, but here we use the recommended way for notebooks, using the `start` and `stop` functions of the tracker. Other options can be found in the [documentation](https://mlco2.github.io/codecarbon/usage.html).\n",
    " The package by default logs data into a CSV file named `emissions.csv` in the current directory.\n",
    "\n",
    "<span style=\"color:DarkRed\"> Q4. What lines have been added int the code `TrainingClassification_codecarbon.py`? Run it and observe the results. \n",
    "    \n",
    "<span style=\"color:DarkRed\"> Q5. The list of supported CPU on codecarbon is available here: https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/hardware/cpu_power.csv . Is your CPU listed? If not, what is the the TDP of your CPU? Note that if the CPU is not listed then codecarbon will estimate the power consumption of the CPUs as 50% of their thermal design power (TDP) using a default TDP average of 85W. Compare you rTDP with this 85W.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAudjSsA8UUs"
   },
   "source": [
    "Information about the infrastructure of the used platform can be seen on the standard output. The output also indicates the energy consumed by the components and the resulting emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88PgXEdio8Fl"
   },
   "source": [
    "> **Remark 1:** CPUs tracking uses RAPL files or Power Gadget (only for INTEL CPUs with root access). The consumption reported by RAPL files or Power Gadget represents the consumption of the whole machine, and not only the process. If CodeCarbon cannot find the software to track the CPUs, then the tool uses the model of CPU to search in a list the corresponding TDP. If the model is unknown, it uses a TDP of 85W. This assumption can lead to reporting values of carbon emissions that are not real.\n",
    "\n",
    "> **Remark 2:** GPUs tracking uses `pynvml` library (only for NVIDIA GPUs). CodeCarbon does not measure consumption of *non-NVIDIA GPUs*. The consumption reported by pynvml represents the consumption of the whole machine, and not only the process.\n",
    "\n",
    "> **Remark 3:** Energy consumption by memory is 0.375W/GB of memory used. If tracking mode is *process*, the memory used by the process is measured via `psutil`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPMki42OmWHd"
   },
   "source": [
    "\n",
    "<span style=\"color:DarkRed\">Q6.  Compare the energy consumption with the one from Green-Algorithms and wattmeter. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scJSnuq5E6ZY"
   },
   "source": [
    "<span style=\"color:DarkRed\"> Q7. What advantages and disadvantages do you see in using each tool?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - Precision vs. consumption\n",
    "\n",
    "Let us consider again the following _scenario 1_.\n",
    "\n",
    "|  Specifications |   | \n",
    "|---|---|\n",
    "|  Execution time | 190h |   \n",
    "| Number of CPUs  |  4 |   \n",
    "| Type of CPUs  | Xeon E5-2683 v4  |  \n",
    "| Number of GPUs  | 4  | \n",
    "| Type of GPUs  |  Tesla V100 |  \n",
    "| Avalaible memory  |  32 Go |  \n",
    "| Location of the server | Orsay,France  |  \n",
    "\n",
    "<span style=\"color:DarkRed\">Q8. In this scenario, what quantity of CO2 emission would have been saved if the training add been stopped after three days? (use Green Algorithms to answer the questions)</span>\n",
    "\n",
    "\n",
    "\n",
    "Let us now consider the following figure from <a name=\"cite_ref-1\"></a>[[1]](#cite_note-1). This figure displays CO2 emitted in kg (in France) by different end-to-end automatic speech recognition models with respect to the word error rate (WER) on the dev sets of LibriSpeech and CommonVoice. The curves exhibit an exponential trend as most of the training time is devoted to slightly reduce the WER. The black and red dots indicate the WER obtained with 50% and 100% of the emitted CO2. On LibriSpeech, 50% of the carbon emissions have been dedicated to reach SOTA results with an improvement of 0.37%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Parcollet.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:DarkRed\">Q9. Which is the best speech recognition model according to WER? When do you think it is reasonable to stop the training of such a model? </span>\n",
    "\n",
    "\n",
    "We will now observe and measure more deeply the consumption while training digit classification. \n",
    "\n",
    "In `TrainingClassification_codecarbon_callback.py`, we add a print function for codecarbon. During training, the progress is displayed on standard output. Once finished, it can be useful to visualize how the model evolved during training.\n",
    "\n",
    "We also define a callback to track energy consumption during training. A callback is created so that at the end of an epoch the energy consumption is verified, and the training is interrupt if the threshold of 0.001kwh was exceeded. Save the intermediate values of energy consumed by the training. You can use the callback method on_epoch_end. \n",
    "\n",
    "\n",
    "<span style=\"color:DarkRed\"> Q10. Observe the file `TrainingClassification_codecarbon_callback.py` to see the differences. \n",
    "Add code to plot curves displaying evolution of accuracy and energy consumption across epochs. According to you: When does it make sense to end the training, considering the accuracy and energy consumption? </span>\n",
    "\n",
    "<span style=\"color:DarkRed\">Q11. Try different threshold values, for example: 0.0001 and 0.01 kwh </span>\n",
    "\n",
    "<span style=\"color:DarkRed\">Q12. Experiment now with offline tracking to specify information on running infrastructure. Try also with other countries (Sweden and Poland for instance). Do you see any differences on when the training stops?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fREPY4X7BGBU"
   },
   "source": [
    "## C. Other Impacts\n",
    "\n",
    "\n",
    " Let us now consider the second scenario taken from  <a name=\"cite_ref-2\"></a>[[2]](#cite_note-2). In this paper, an estimation of the carbon footprint of the language model Bloom is proposed. \n",
    " \n",
    " <span style=\"color:DarkRed\">Q13. Use Green Algorithm to evaluate carbon emissions and compare with the results from the paper (section 4.2).   </span>\n",
    "\n",
    "|  Specifications |   | \n",
    "|---|---|\n",
    "|  Execution time | 118 days (2 832 hours), 5 hours, 41 mins  |   \n",
    "| Type of GPUs  | Nvidia A100  |  \n",
    "| Number of GPUs  | 416  | \n",
    "| TDP of CPU |  400 |  \n",
    "| Avalaible memory  |  33 280 GB |  \n",
    "| Location of the server | Orsay, France  |   \n",
    "| PUE | 1.2  |  \n",
    "| Carbon intensity of the energy grid | 57 gCO2eq/kWh |\n",
    "\n",
    "<span style=\"color:DarkRed\">Q14. In this paper observe table 2. What part of power consumption is computed by Green Algorithm? </span>\n",
    "    \n",
    "<span style=\"color:DarkRed\">Q15. Read section 4.4 of this paper. Are there some information that you found surprising regarding the deployment of the model?</span>\n",
    "    \n",
    "<span style=\"color:DarkRed\">Q16. According to your knowledge, do all these tools take into account all environmental impacts? </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlyeARERE6ZY"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>[1] [^](#cite_ref-1)  Titouan Parcollet, Mirco Ravanelli. The Energy and Carbon Footprint of Training End-to-End Speech Recognizers. 2021\n",
    "\n",
    "\n",
    "<a name=\"cite_note-2\"></a>[2] [^](#cite_ref-2)  A.S. Luccioni, S. Viguier, A.-L. Ligozat. Estimating the Carbon Footprint of BLOOM, a 176B Parameter, arXiv:2211.02001, 2022"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "full-user-track-cell": "",
  "full-user-track-date": "",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
