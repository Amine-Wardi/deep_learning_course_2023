{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmXjwSCv_quF"
      },
      "source": [
        "# IS319 - Deep Learning\n",
        "\n",
        "## TP3 - Recurrent neural networks\n",
        "\n",
        "Credits: Andrej Karpathy\n",
        "\n",
        "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbyFcdlF_kQh",
        "outputId": "9474c25f-8ad8-43e0-caae-bb0950f57083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizer\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distributions\n",
        "import matplotlib.pyplot as plt\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    # else \"mps\"\n",
        "    # if torch.backends.mps.is_available() # For macOS\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f'Using {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQjJoF7-_z4m"
      },
      "source": [
        "## 1. Text data preprocessing\n",
        "\n",
        "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ewBhWN_uD0",
        "outputId": "4b86f888-2240-4620-d3bf-4f60f308fac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset `shakespeare.txt` contains 95665 characters.\n",
            "Excerpt of the dataset:\n",
            "    SONNETS\n",
            "\n",
            "\n",
            "\n",
            "TO THE ONLY BEGETTER OF\n",
            "THESE INSUING SONNETS\n",
            "MR. W. H. ALL HAPPINESS\n",
            "AND THAT ETERNITY\n",
            "PROMISED BY\n",
            "OUR EVER-LIVING POET WISHETH\n",
            "THE WELL-WISHING\n",
            "ADVENTURER IN\n",
            "SETTING FORTH\n",
            "T. T.\n",
            "\n",
            "\n",
            "I.\n",
            "\n",
            "FROM fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou, contracted to thine own bright eyes,\n",
            "Feed'st thy light'st flame with self-substantial fuel,\n",
            "Making a famine where abundance lies,\n",
            "Thyself thy foe, to thy sweet self too cruel.\n",
            "Thou that art now the world's fresh ornament\n",
            "And only herald to the gaudy spring,\n",
            "Within thine own bud buriest thy content\n",
            "And, tender churl, makest waste in niggarding.\n",
            "  Pity the world, or else this glutton be,\n",
            "  To eat the world's due, by the grave and thee.\n",
            "\n",
            "II.\n",
            "\n",
            "When forty winters shall beseige thy brow,\n",
            "And dig deep trenches in thy beauty's field,\n",
            "Thy youth's proud livery, so gazed on now,\n",
            "Will be a tatter'd weed, of small worth held:\n",
            "Then being ask'd where all thy beauty lies,\n",
            "Where all the treasure of thy lusty days,\n",
            "To say, within thine own deep-sunken eyes,\n",
            "Were an all-eating shame and thriftless praise.\n",
            "How much more praise deserved thy beauty's use,\n",
            "If thou couldst answer 'This fair child of mine\n",
            "Shall sum my count and make my old excuse,'\n",
            "Proving his beauty by succession thine!\n",
            "  This were to be new made when thou art old,\n",
            "  And see thy blood warm when thou feel'st it cold.\n",
            "\n",
            "III.\n",
            "\n",
            "Look in thy glass, and tell the face thou viewest\n",
            "Now is the time that face should form another;\n",
            "Whose fresh repair if now thou not renewest,\n",
            "Thou dost beguile the world, unbless some mother.\n",
            "For where is she so fair whose unear'd womb\n",
            "Disdains the tillage of thy husbandry?\n",
            "Or who is he so fond will be the tomb\n",
            "Of his self-love, to stop posterity?\n",
            "Thou art thy mother's glass, and she in thee\n",
            "Calls back the lovely April of her prime:\n",
            "So thou through windows of thine age shall see\n",
            "Despite of wrinkles this thy golden time.\n",
            "  But if thou\n"
          ]
        }
      ],
      "source": [
        "dir_datasets = \"./\"\n",
        "# text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
        "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
        "text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
        "# text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
        "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
        "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
        "\n",
        "text_data = open(dir_datasets+text_data_fname, 'r').read()\n",
        "text_data = text_data # use a small subset\n",
        "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
        "print('Excerpt of the dataset:')\n",
        "print(text_data[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wExdlSewAnix"
      },
      "source": [
        "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFxMMGlMAoLd",
        "outputId": "6b733987-d2b2-4120-b974-123a8ca772ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ', 'S', 'O', 'N', 'E', 'T', '\\n', 'H', 'L', 'Y', 'B', 'G', 'R', 'F', 'I', 'U', 'M', '.', 'W', 'A', 'P', 'D', 'V', '-', 'f', 'a', 'i', 'r', 'e', 's', 't', 'c', 'u', 'w', 'd', 'n', ',', 'h', 'b', 'y', \"'\", 'o', 'm', 'g', 'v', 'p', 'l', ':', 'k', 'z', 'x', '!', ';', '?', 'C', 'q', 'j', 'X', 'K', 'J', '[', ']']\n",
            "{' ': 0, 'S': 1, 'O': 2, 'N': 3, 'E': 4, 'T': 5, '\\n': 6, 'H': 7, 'L': 8, 'Y': 9, 'B': 10, 'G': 11, 'R': 12, 'F': 13, 'I': 14, 'U': 15, 'M': 16, '.': 17, 'W': 18, 'A': 19, 'P': 20, 'D': 21, 'V': 22, '-': 23, 'f': 24, 'a': 25, 'i': 26, 'r': 27, 'e': 28, 's': 29, 't': 30, 'c': 31, 'u': 32, 'w': 33, 'd': 34, 'n': 35, ',': 36, 'h': 37, 'b': 38, 'y': 39, \"'\": 40, 'o': 41, 'm': 42, 'g': 43, 'v': 44, 'p': 45, 'l': 46, ':': 47, 'k': 48, 'z': 49, 'x': 50, '!': 51, ';': 52, '?': 53, 'C': 54, 'q': 55, 'j': 56, 'X': 57, 'K': 58, 'J': 59, '[': 60, ']': 61}\n",
            "{0: ' ', 1: 'S', 2: 'O', 3: 'N', 4: 'E', 5: 'T', 6: '\\n', 7: 'H', 8: 'L', 9: 'Y', 10: 'B', 11: 'G', 12: 'R', 13: 'F', 14: 'I', 15: 'U', 16: 'M', 17: '.', 18: 'W', 19: 'A', 20: 'P', 21: 'D', 22: 'V', 23: '-', 24: 'f', 25: 'a', 26: 'i', 27: 'r', 28: 'e', 29: 's', 30: 't', 31: 'c', 32: 'u', 33: 'w', 34: 'd', 35: 'n', 36: ',', 37: 'h', 38: 'b', 39: 'y', 40: \"'\", 41: 'o', 42: 'm', 43: 'g', 44: 'v', 45: 'p', 46: 'l', 47: ':', 48: 'k', 49: 'z', 50: 'x', 51: '!', 52: ';', 53: '?', 54: 'C', 55: 'q', 56: 'j', 57: 'X', 58: 'K', 59: 'J', 60: '[', 61: ']'}\n",
            "tensor([ 0,  0,  0,  ..., 44, 28, 17])\n"
          ]
        }
      ],
      "source": [
        "# Create the vocabulary and the two mapping dictionaries\n",
        "def create_vocab(text):\n",
        "    vocab, ctoi, itoc = [], {}, {}\n",
        "    for character in text :\n",
        "        if character not in vocab :\n",
        "            vocab += [character]\n",
        "            ctoi[character] = vocab.index(character)\n",
        "            itoc[vocab.index(character)] = character\n",
        "    return vocab, ctoi, itoc\n",
        "\n",
        "\n",
        "# Implement the function converting text to tensor\n",
        "def text_to_tensor(text, ctoi):\n",
        "    arr = np.zeros((len(text)))\n",
        "    for idx, char in enumerate(text):\n",
        "        arr[idx] = ctoi[char]\n",
        "    return torch.tensor(arr, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "# Implement the function converting tensor to text\n",
        "def tensor_to_text(tensor, itoc):\n",
        "    arr = tensor.cpu().detach().numpy()\n",
        "    text = \"\"\n",
        "    for elm in arr :\n",
        "        text+=itoc[elm]\n",
        "    return text\n",
        "\n",
        "# Apply your functions to some text data\n",
        "\n",
        "vocab, ctoi, itoc = create_vocab(text_data)\n",
        "\n",
        "print(vocab)\n",
        "print(ctoi)\n",
        "print(itoc)\n",
        "\n",
        "example_tensor = text_to_tensor(text_data, ctoi)\n",
        "print(example_tensor)\n",
        "\n",
        "example_text = tensor_to_text(example_tensor, itoc)\n",
        "# verify integrity\n",
        "assert example_text == text_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpAmk7FZAp41"
      },
      "source": [
        "## 2. Setup a character-level recurrent neural network\n",
        "\n",
        "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8OWfLIWAvGA",
        "outputId": "2d39bc45-7534-4de9-d581-979ba9893da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2131, -1.1736,  0.0767,  ..., -0.3563, -0.6911, -0.5352],\n",
            "        [ 0.2131, -1.1736,  0.0767,  ..., -0.3563, -0.6911, -0.5352],\n",
            "        [ 0.2131, -1.1736,  0.0767,  ..., -0.3563, -0.6911, -0.5352],\n",
            "        ...,\n",
            "        [ 1.5456,  0.2271, -2.0628,  ...,  0.2706,  0.0348,  0.8645],\n",
            "        [ 0.2865,  0.3737, -0.7090,  ..., -0.5581, -1.3705,  0.4577],\n",
            "        [ 1.3353,  0.6194, -0.1677,  ...,  0.6436,  1.5125, -0.0368]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([95665, 16])\n"
          ]
        }
      ],
      "source": [
        "# n_vocab : the total number of unique indices that the embedding layer can handle.\n",
        "# n_dim : the size of the vector space in which the indices will be embedded.\n",
        "n_vocab, n_dim = len(vocab), 16\n",
        "\n",
        "# initiate the Embedding layer\n",
        "emb_layer = nn.Embedding(n_vocab, embedding_dim=n_dim)\n",
        "\n",
        "# given the example tensor generate an embedding of each index (indirectly character) of the text\n",
        "emb_data = emb_layer(example_tensor)\n",
        "\n",
        "print(emb_data)\n",
        "print(emb_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tfWqlErAw05"
      },
      "source": [
        "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIavuw40AyXL",
        "outputId": "f90a7867-9800-447f-ba84-698e13188e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([95665, 16]) tensor([[ 0.8377, -0.2862,  0.4819,  ..., -0.6358,  0.7918, -0.0943],\n",
            "        [ 0.8505, -0.1561,  0.5117,  ..., -0.2866,  0.5960, -0.1933],\n",
            "        [ 0.8643, -0.2143,  0.4815,  ..., -0.3118,  0.5495, -0.2887],\n",
            "        ...,\n",
            "        [ 0.3187, -0.2441,  0.3486,  ...,  0.6295,  0.5231,  0.5456],\n",
            "        [-0.5798, -0.3524, -0.6550,  ...,  0.4541,  0.8113, -0.5964],\n",
            "        [ 0.5324, -0.0520, -0.5687,  ...,  0.4035,  0.1437, -0.4652]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "tensor([[ 0.5324, -0.0520, -0.5687,  0.6769, -0.5412, -0.4323,  0.0864,  0.2950,\n",
            "          0.7322,  0.8644,  0.3394,  0.5966,  0.8700,  0.4035,  0.1437, -0.4652]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "input_size, hidden_size = 16, 16\n",
        "rnn_layer = nn.RNN(input_size, hidden_size)\n",
        "\n",
        "# Initialize the hidden state\n",
        "hidden_state = torch.zeros(1, hidden_size)\n",
        "\n",
        "# run the embedded data through the RNN\n",
        "output_sequence, final_hidden_state = rnn_layer(emb_data, hidden_state)\n",
        "\n",
        "print(output_sequence.shape, output_sequence)\n",
        "print(final_hidden_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL73a2l5A0s6"
      },
      "source": [
        "## Answer :\n",
        "The `output_sequence` represents the output of the RNN at each time step when it processes an input sequence. It is a tensor that contains the predicted value for each hidden state at each time step in the sequence. And the `final_hidden_state` is, as shown, the final hidden state of the layer.\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOP6ZfNaA4tu"
      },
      "source": [
        "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpgplzg5A38a",
        "outputId": "3ddc2afc-e810-41ef-9de8-86f69ee30056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 50/900, Loss: 3.5179476737976074\n",
            "Iteration 100/900, Loss: 1.9047702550888062\n",
            "Iteration 150/900, Loss: 0.8824202418327332\n",
            "Iteration 200/900, Loss: 0.4462552070617676\n",
            "Iteration 250/900, Loss: 0.26752108335494995\n",
            "Iteration 300/900, Loss: 0.18084551393985748\n",
            "Iteration 350/900, Loss: 0.13251593708992004\n",
            "Iteration 400/900, Loss: 0.10254301130771637\n",
            "Iteration 450/900, Loss: 0.08233128488063812\n",
            "Iteration 500/900, Loss: 0.0679052546620369\n",
            "Iteration 550/900, Loss: 0.05718304589390755\n",
            "Iteration 600/900, Loss: 0.0489538349211216\n",
            "Iteration 650/900, Loss: 0.04247773066163063\n",
            "Iteration 700/900, Loss: 0.03727369382977486\n",
            "Iteration 750/900, Loss: 0.03301691263914108\n",
            "Iteration 800/900, Loss: 0.029481202363967896\n",
            "Iteration 850/900, Loss: 0.026505444198846817\n",
            "Iteration 900/900, Loss: 0.023972200229763985\n"
          ]
        }
      ],
      "source": [
        "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
        "input_seq = torch.arange(0, 40).long()\n",
        "target_seq = (input_seq+1).clone() \n",
        "\n",
        "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
        "def train_overfit(model, input_seq, target_seq, optim, loss_function, n_iters=200, learning_rate=0.002, check_iter=50):\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        # Forward pass\n",
        "        output, hidden = model(input_seq.unsqueeze(0))  # Add batch dimension\n",
        "        loss = loss_function(output.squeeze(0), target_seq)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if (i + 1) % check_iter == 0:\n",
        "            print(f\"Iteration {i + 1}/{n_iters}, Loss: {loss.item()}\")\n",
        "\n",
        "# Initialize a model and make it overfit the input sequence\n",
        "class simpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(simpleRNN, self).__init__()\n",
        "        self.emb = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        output, hidden = self.rnn(x)\n",
        "        output = self.lin(output)\n",
        "        return output, hidden\n",
        "\n",
        "# Initialize the model, loss function, and optimizer (Adam)\n",
        "input_size, hidden_size, output_size, learning_rate = 200, 16, 200, 0.002\n",
        "model = simpleRNN(input_size, hidden_size, output_size)\n",
        "optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train_overfit(model, input_seq, target_seq, optim, loss_function, n_iters=900)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK0NzXOuA9Sc"
      },
      "source": [
        "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clAfcYtRA-jO",
        "outputId": "0f254842-5c99-4f0d-a1ee-22f4df8944ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 50/900, Loss: 3.439159393310547\n",
            "Iteration 100/900, Loss: 1.9932798147201538\n",
            "Iteration 150/900, Loss: 1.0686637163162231\n",
            "Iteration 200/900, Loss: 0.5870904922485352\n",
            "Iteration 250/900, Loss: 0.35546037554740906\n",
            "Iteration 300/900, Loss: 0.23519167304039001\n",
            "Iteration 350/900, Loss: 0.16645558178424835\n",
            "Iteration 400/900, Loss: 0.12433061748743057\n",
            "Iteration 450/900, Loss: 0.09685086458921432\n",
            "Iteration 500/900, Loss: 0.07793130725622177\n",
            "Iteration 550/900, Loss: 0.0642877146601677\n",
            "Iteration 600/900, Loss: 0.05410720780491829\n",
            "Iteration 650/900, Loss: 0.04632621258497238\n",
            "Iteration 700/900, Loss: 0.0402187816798687\n",
            "Iteration 750/900, Loss: 0.03532419353723526\n",
            "Iteration 800/900, Loss: 0.031332828104496\n",
            "Iteration 850/900, Loss: 0.028027767315506935\n",
            "Iteration 900/900, Loss: 0.025253888219594955\n"
          ]
        }
      ],
      "source": [
        "class CharRNN(simpleRNN):\n",
        "    def predict_argmax(self, context_tensor, n_predictions):\n",
        "        predictions, hidden = [], None\n",
        "\n",
        "        # Use the context tensor to apply the forward pass for the context tensor\n",
        "        for char_index in context_tensor:\n",
        "            output, hidden = self.forward(char_index.unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "        # Predict the next n_predictions indices\n",
        "        for _ in range(n_predictions):\n",
        "            # Forward pass with the last index and hidden state\n",
        "            output, hidden = self.forward(context_tensor[-1].unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "            # Get the index of the predicted index using argmax\n",
        "            predicted_index = output.squeeze(0).argmax().item()\n",
        "            predictions.append(predicted_index)\n",
        "\n",
        "            # Update the context tensor with the new prediction\n",
        "            context_tensor = torch.cat((context_tensor, torch.tensor([predicted_index])))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Initialize a model and make it overfit as above\n",
        "# Then verify your overfitting by predicting characters given some context\n",
        "model = CharRNN(input_size, hidden_size, output_size)\n",
        "optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "train_overfit(model, input_seq, target_seq, optim, loss_function, n_iters=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOP-VlA9hXDP",
        "outputId": "790e5271-fab6-4b7b-c410-968a8ecd897c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[35, 36, 37]\n"
          ]
        }
      ],
      "source": [
        "# predict 3 indices\n",
        "print(model.predict_argmax(input_seq[:35], n_predictions=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bml7Vn4PBEoJ"
      },
      "source": [
        "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator.\n",
        "\n",
        "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiXNLU2TBD0E",
        "outputId": "ce5cd123-3f44-49bc-f9de-f198ac1b5074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 50/900, Loss: 3.517578125\n",
            "Iteration 100/900, Loss: 1.9130096435546875\n",
            "Iteration 150/900, Loss: 0.9699044227600098\n",
            "Iteration 200/900, Loss: 0.5434590578079224\n",
            "Iteration 250/900, Loss: 0.3417798578739166\n",
            "Iteration 300/900, Loss: 0.23311099410057068\n",
            "Iteration 350/900, Loss: 0.1664937287569046\n",
            "Iteration 400/900, Loss: 0.12370067834854126\n",
            "Iteration 450/900, Loss: 0.09567565470933914\n",
            "Iteration 500/900, Loss: 0.07620358467102051\n",
            "Iteration 550/900, Loss: 0.062127985060214996\n",
            "Iteration 600/900, Loss: 0.05183090642094612\n",
            "Iteration 650/900, Loss: 0.044098202139139175\n",
            "Iteration 700/900, Loss: 0.038113728165626526\n",
            "Iteration 750/900, Loss: 0.03336089104413986\n",
            "Iteration 800/900, Loss: 0.029505740851163864\n",
            "Iteration 850/900, Loss: 0.026323989033699036\n",
            "Iteration 900/900, Loss: 0.023659957572817802\n"
          ]
        }
      ],
      "source": [
        "class CharRNN(CharRNN):\n",
        "    def predict_proba(self, context_tensor, n_predictions):\n",
        "        predictions, hidden = [], None\n",
        "\n",
        "        # Use the context tensor to apply the forward pass for the context tensor\n",
        "        for char_index in context_tensor:\n",
        "            output, hidden = self.forward(char_index.unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "        # Predict the next n_predictions characters by sampling from the distribution\n",
        "        for _ in range(n_predictions):\n",
        "            # Forward pass with the last character and hidden state\n",
        "            output, hidden = self.forward(context_tensor[-1].unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "            # Use Categorical distribution to sample from the predicted probabilities\n",
        "            categorical_dist = distributions.Categorical(logits=output.squeeze(0))\n",
        "            predicted_index = categorical_dist.sample().item()\n",
        "            predictions.append(predicted_index)\n",
        "\n",
        "            # Update the context tensor with the new prediction\n",
        "            context_tensor = torch.cat((context_tensor, torch.tensor([predicted_index])))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Verify that your predictions are not deterministic anymore\n",
        "model = CharRNN(input_size, hidden_size, output_size)\n",
        "optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "train_overfit(model, input_seq, target_seq, optim, loss_function, n_iters=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ssVtW66iqyT",
        "outputId": "70ccedcf-7cd3-4036-ab83-695bc55d8fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[30, 31, 32]\n"
          ]
        }
      ],
      "source": [
        "# predict 3 indices\n",
        "print(model.predict_proba(input_seq[:30], n_predictions=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhi7t0hKBJNM"
      },
      "source": [
        "## 3. Train the RNN model on text data\n",
        "\n",
        "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WIC8rES-BLrU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/500, Loss: 2.170841693878174\n",
            "Epoch 20/500, Loss: 2.259021520614624\n",
            "Epoch 30/500, Loss: 1.9983110427856445\n",
            "Epoch 40/500, Loss: 2.1692848205566406\n",
            "Epoch 50/500, Loss: 2.2065696716308594\n",
            "Epoch 60/500, Loss: 2.2631304264068604\n",
            "Epoch 70/500, Loss: 2.474623918533325\n",
            "Epoch 80/500, Loss: 2.31829833984375\n",
            "Epoch 90/500, Loss: 2.199216365814209\n",
            "Epoch 100/500, Loss: 2.444514751434326\n",
            "Epoch 110/500, Loss: 2.188258171081543\n",
            "Epoch 120/500, Loss: 2.559018850326538\n",
            "Epoch 130/500, Loss: 2.0517821311950684\n",
            "Epoch 140/500, Loss: 2.432243824005127\n",
            "Epoch 150/500, Loss: 2.6098146438598633\n",
            "Epoch 160/500, Loss: 2.2503933906555176\n",
            "Epoch 170/500, Loss: 2.2001070976257324\n",
            "Epoch 180/500, Loss: 2.5504813194274902\n",
            "Epoch 190/500, Loss: 2.6394217014312744\n",
            "Epoch 200/500, Loss: 2.303760051727295\n",
            "Epoch 210/500, Loss: 2.5553977489471436\n",
            "Epoch 220/500, Loss: 2.3157849311828613\n",
            "Epoch 230/500, Loss: 2.4115264415740967\n",
            "Epoch 240/500, Loss: 2.5328922271728516\n",
            "Epoch 250/500, Loss: 2.238132953643799\n",
            "Epoch 260/500, Loss: 2.475147008895874\n",
            "Epoch 270/500, Loss: 2.446547269821167\n",
            "Epoch 280/500, Loss: 2.5358376502990723\n",
            "Epoch 290/500, Loss: 2.5418975353240967\n",
            "Epoch 300/500, Loss: 2.1847665309906006\n",
            "Epoch 310/500, Loss: 2.370034694671631\n",
            "Epoch 320/500, Loss: 2.2355833053588867\n",
            "Epoch 330/500, Loss: 2.11080265045166\n",
            "Epoch 340/500, Loss: 2.3876359462738037\n",
            "Epoch 350/500, Loss: 2.418609142303467\n",
            "Epoch 360/500, Loss: 2.3067853450775146\n",
            "Epoch 370/500, Loss: 2.117593288421631\n",
            "Epoch 380/500, Loss: 2.3477158546447754\n",
            "Epoch 390/500, Loss: 2.442966938018799\n",
            "Epoch 400/500, Loss: 2.335871696472168\n",
            "Epoch 410/500, Loss: 2.4698128700256348\n",
            "Epoch 420/500, Loss: 1.9594805240631104\n",
            "Epoch 430/500, Loss: 2.007578134536743\n",
            "Epoch 440/500, Loss: 2.5970101356506348\n",
            "Epoch 450/500, Loss: 2.1190476417541504\n",
            "Epoch 460/500, Loss: 2.3887436389923096\n",
            "Epoch 470/500, Loss: 2.5533089637756348\n",
            "Epoch 480/500, Loss: 2.313413619995117\n",
            "Epoch 490/500, Loss: 2.397864580154419\n",
            "Epoch 500/500, Loss: 2.0988643169403076\n"
          ]
        }
      ],
      "source": [
        "# Create the text dataset, compute its mappings and convert it to tensor\n",
        "vocab, ctoi, itoc = create_vocab(text_data)\n",
        "data_tensor = text_to_tensor(text_data, ctoi)\n",
        "seq_len = 20\n",
        "# Initialize training parameters\n",
        "input_size, hidden_size, output_size, learning_rate = len(vocab), 64, len(vocab), 0.005\n",
        "\n",
        "# Initialize a character-level RNN model\n",
        "model = CharRNN(input_size, hidden_size, output_size).to(device)\n",
        "optim = optimizer.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setup the training loop\n",
        "# Regularly record the loss and sample from the model to monitor what is happening\n",
        "# YOUR CODE HERE\n",
        "def train_loop(model, data_tensor, seq_len, n_epochs, optim, loss_function, device=device):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        # Randomly choose a starting point for each epoch\n",
        "        start_index = np.random.randint(0, data_tensor.size(0) - seq_len - 1)\n",
        "        \n",
        "        for i in range(start_index, data_tensor.size(0) - seq_len, seq_len):\n",
        "            input_seq = data_tensor[i:i+seq_len].unsqueeze(0).to(device)\n",
        "            target_seq = data_tensor[i+1:i+seq_len+1].to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            output, hidden = model(input_seq)\n",
        "\n",
        "            loss = loss_function(output.squeeze(0), target_seq)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "n_epochs = 500\n",
        "train_loop(model, data_tensor, seq_len, n_epochs, optim, loss_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UryaOe-JBN7A"
      },
      "source": [
        "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0YNuDwx2BPEz"
      },
      "outputs": [],
      "source": [
        "start_text = \"to the only begetter of these insuing\"\n",
        "model.eval()\n",
        "generated_text = start_text\n",
        "n_chars = 10\n",
        "\n",
        "with torch.no_grad():\n",
        "    # hidden_state = torch.zeros(1, 1, hidden_size).to(device)\n",
        "    input_seq = text_to_tensor(start_text, ctoi).to(device)\n",
        "    predicted_indices = model.predict_proba(input_seq, n_predictions=n_chars)\n",
        "    \n",
        "    for idx in predicted_indices :\n",
        "        if idx < len(vocab) :\n",
        "            generated_text += itoc[idx]\n",
        "        else : \n",
        "            print(idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "to the only begetter of these insuinghino qutwa\n"
          ]
        }
      ],
      "source": [
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tafoNLnIBQUY"
      },
      "source": [
        "## Answer :\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHwYCrJ9BUc3"
      },
      "source": [
        "## 4. Experiment with different RNN architectures\n",
        "\n",
        "**(Question)** Experiment with different RNN architecures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPKB7NtbBTOz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klKuqVJTBX0A"
      },
      "source": [
        "## Answer :\n",
        "****"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
